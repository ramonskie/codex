# OpenStack Codex Walkthrough

## Overview

Welcome to the Stark & Wayne guide to deploying Cloud Foundry on OpenStack.
This guide provides the steps to create authentication credentials,
generate the underlying cloud infrastructure, then use Terraform to prepare a bastion
host.

From this bastion, we setup a special BOSH Director we call the **proto-BOSH**
server where software like Vault, Concourse, Bolo and SHIELD are setup in order
to give each of the environments created after the **proto-BOSH** key benefits of:

* Secure Credential Storage
* Pipeline Management
* Monitoring Framework
* Backup and Restore Datastores

Once the **proto-BOSH** environment is setup, the child environments will have
the added benefit of being able to update their BOSH software as a release,
rather than having to re-initialize with `bosh-init`.

This also increases the resiliency of all BOSH Directors through monitoring and
backups with software created by Stark & Wayne's engineers.

And visibility into the progress and health of each application, release, or
package is available through the power of Concourse pipelines.

![Levels of Bosh][levels_of_bosh]

In the above diagram, BOSH (1) is the **proto-BOSH**, while BOSH (2) and BOSH (3)
are the per-site BOSH Directors. Note that it is the proto-BOSH (`dc01-proto-openvdc`)
that deploys Vault, Concourse, Bolo, SHIELD as well as the other BOSH directors.

Now it's time to setup the credentials.

## Setup Credentials

To start deploying the infrastructure, the first thing you need to do is create
an OpenStack user and give it admin access to a new tenant.

1. Log into Horizon as the **admin** user.
2. Under **Identity --> Projects**, create a new project and set the quotas to
   accommodate the environment(s) that will be deployed to this project.

   Keep in mind:  If setting specific quotas, you will need to ask for more than
   the total resources that you are going to end up with.  This is to account for
   transient instances, such as BOSH errands and Compilation instances.

   For a typical development environment:
   * **VCPUs** - 200
   * **Instances**  - 100
   * **Volumes**    - 25
   * **RAM (MB)**   - 512000

   For a typical staging / production environment:
   * **VCPUs** - 200
   * **Instances**  - 100
   * **Volumes**    - 25
   * **RAM (MB)**   - 512000

3. Under **Identity --> Users**, create a new user.  Give the user **admin** access
   to the new project.

### Generate OpenStack Key Pair

The **User Name**, **Password**, and **Tenant Name** (same as **Project Name**) are
used to get access to the OpenStack Services by BOSH.  Next, we'll need to create a
**Key Pair**.  This will be used as we bring up the initial bastion host instances,
and is the SSH key you'll use to connect from your local machine to the bastion.

**NOTE:** Due to a bug in Liberty and Mitaka you cannot use keys generated by Nova
  with bosh-init: you must generate the key pair separately with `ssh-keygen`.
  Releases of OpenStack before Liberty and after Mitaka are not impacted by this
  issue, so keys can either be generated in OpenStack or with `ssh-keygen`. A short
  description of this error is available on the [bosh.io openstack-cpi doc](https://bosh.io/docs/openstack-cpi.html)
  with a link to the bug report.

1. Log into Horizon as the user that has admin access to the project in question.
 * Make sure you are in the correct project (top-left corner of the Horizon
 UI) or the key pair you generate will not work.

2. Under **Project --> Compute --> Access & Security**, head to the **Key Pairs**
   tab.

3. **To create a key in OpenStack:** Click **Create Key Pair** and give the
   key a suitable name, e.g. `bosh`. When the key pair is generated it will be
   immediately available for use in OpenStack and its `*.pem` file will download.<br /><br />
   **To use a key pair created with `ssh-keygen`:** Click **Import Key Pair**,
   give the key a suitable name (e.g. `bosh`), and paste in the public key.

4. `*.pem` / key files should have their permissions set to `0600`.

5. Decide where you want this file to be.  All `*.pem` files are ignored in the
   codex repository.  So you can either move this file to the same folder as
  `CODEX_ROOT/terraform/openstack` or move it to a place you normally keep SSH keys

## Stand Up A Public Network
Though Terraform is used to stand up most of the networking and initial infrastructure,
it does not set up the publicly facing network that is used to provide external
access to instances and floating IP addresses.  To do this, use `neutron net-create`,
`neutron subnet-create`, to create the public network and allocate the pool of publicly
facing IP addresses, some of which will be used as floating IP's.

Some sample commands are as follows (these will vary depending on the physical structure
  of your network, the configuration of neutron, and whether or not you are using a
  3rd party SDN plugin):
```
neutron net-create public --provider:network_type vlan --provider:physical_network physnet1 --provider:segmentation_id 100 --tenant-id=3c155e766d1d44718cd35765c709fae1 --shared --router:external=True
neutron subnet-create public 	192.168.10.0/24 --disable-dhcp --allocation-pool start=192.168.10.110,end=192.168.10.250 --gateway 192.168.10.1
```

Make note of the public network's UUID.  It will be needed in the next step.

## Use Terraform

Once the requirements for OpenStack are met, we can put it all together and build out
your shiny new networks, routers, security groups and bastion host. Before we begin,
copy the contents of `CODEX_ROOT/terraform/openstack` to your own Github repository
and then change to that directory.

The configuration directly matches the [Network Plan][netplan] for the demo
environment.  When deploying in other environments like production, some tweaks
or rewrites may need to be made.

### Variable File

Create an `openstack.tfvars` file with the following configurations (substituting your
actual values) all the other configurations have default setting in the
`openstack.tf` file.

```
tenant_name = "cf"
user_name = "cfadmin"
password = "putyourpasswordhere"
auth_url = "http://identity.mydatacenter.io:5000/v2.0"
key_pair = "bosh"
region = "os-dc1"
pub_net_uuid = "09b03d93-45f8-4bea-b3b8-7ad9169f23d5"
```

If you need to change the region or subnet, you can override the defaults
by adding:

```
region     = "RegionTwo"
network    = "10.42"
```

To see what variables can be overridden in `openstack.tfvars`, and their default
values, please look at the top of `openstack.tf`.

You may change some default settings according to the real cases you are
working on. For example, you can change `flavor_id` (default is `3`, which is
m1.medium) in `openstack.tf` to something larger if the bastion would require a
high workload.

### Build Resources

As a quick pre-flight check, run `make manifest` to compile your Terraform plan
and suss out any issues with naming, missing variables, configuration, etc.:

```
$ make manifest
terraform get -update
terraform plan -var-file openstack.tfvars -out openstack.tfplan
Refreshing Terraform state prior to plan...

<snip>

Plan: 129 to add, 0 to change, 0 to destroy.
```

If everything worked out you should see a summary of the plan.  If this is the
first time you've done this, all of your changes should be additions.  The
numbers may differ from the above output, and that's okay.

Now, to pull the trigger, run `make deploy`:

```
$ make deploy
```

Terraform will connect to OpenStack using the credentials you provided in the \*.tfvars
file and spin up all the things it needs.  When it finishes, you should be left
with a bunch of subnets, security groups, and a bastion host.

**NOTE:** Circling back around, once you have deployed your Terraform configuration
you will need to push it to your Github repository. When you ran `make manifest`
Terraform created a \*.tfplan file which is based on not only the current \*.tf
and \*.tfvars files but also on the \*.tfstate file from the last run. If there is
no \*.tfstate file then Terraform will assume you are starting a new configuration.
When making changes in the future, you'll want to make sure to push them after the
`make deploy` to ensure that the \*.tfstate file you have in Github is current and correct.

### Connect to Bastion

You'll use the **Key Pair** `*.pem` or `ssh-keygen` generated file that was stored from the
[Generate OpenStack Key Pair](openstack.md#generate-openstack-key-pair) step before as your credential
to connect.

In forming the SSH connection command, use the `-i` flag to give SSH the path to
the `IdentityFile`.  The default user on the bastion server is `ubuntu`.  This
will change in a little bit though when we create a new user, so don't get too
comfy.

```
$ ssh -i ~/.ssh/bosh ubuntu@192.168.10.117
```

### Add User

Once on the bastion host, you'll need to install the `jumpbox` script:

```
sudo curl -o /usr/local/bin/jumpbox \
  https://raw.githubusercontent.com/starkandwayne/jumpbox/master/bin/jumpbox
sudo chmod 0755 /usr/local/bin/jumpbox
```

To check if `jumpbox` was installed correctly, try checking the version:

```
$ jumpbox -v
jumpbox v50
```

[This script installs][jumpbox] some useful utilities such as `jq`, `spruce`, `safe`,
and `genesis` - all of which will be important when we start using the bastion host
to do deployments.

**NOTE**: Try not to confuse the `jumpbox` script with the jumpbox _BOSH release_.
The _BOSH release_ can be used as part of a deployment whereas the script is
run directly on the bastion host.

In order to have the dependencies for the `bosh_cli` we need to create a user.
As part of creating a user you will be prompted for their git configuration, which
will be useful when we use Genesis templates to create deployments later.

Using named accounts will also provide auditing (via the `sudo` logs) as well as
isolation (people won't step on each others toes on the filesystem) and
customization (everyone gets to set their own prompt / shell / `$EDITOR`).

Let's add a user with `jumpbox useradd`:

```
$ jumpbox useradd
Full name: J User
Username:  juser
Enter the public key for this user's .ssh/authorized_keys file:
You should run `jumpbox user` now, as juser:
  su - juser
  jumpbox user
```

### Setup User

After you've added the user, **be sure you follow up and setup the user** before
going any further.

Use the `su - juser` command to switch to the user.  And run `jumpbox user`
to install all dependent packages.

```
$ su - juser
$ jumpbox user
```

The following warning may show up when you run `jumpbox user`:
```
 * WARNING: You have '~/.profile' file, you might want to load it,
    to do that add the following line to '/home/juser/.bash_profile':

      source ~/.profile
```

In this case, please follow the `WARNING` message, otherwise you may see the following message when you run `jumpbox` command even if you already installed everything when you run `jumpbox user`.

```
ruby not installed
rvm not installed
bosh not installed
```

### SSH Config

On your local computer, setup an entry in the `~/.ssh/config` file for your
bastion host - substituting the correct IP and SSH key.

```
Host bastion
  Hostname 52.43.51.197
  IdentityFile ~/.ssh/id_rsa
  User juser
```

### Test Login

After you've logged in as `ubuntu` once, created your user, logged out, and
configured your SSH config, you'll be ready to try to connect via the `Host`
alias.

```
$ ssh bastion
```

If you can login and run `jumpbox` and everything returns green, everything's
ready to continue.

```
$ jumpbox

<snip>

>> Checking jumpbox installation
jumpbox installed - jumpbox v49
ruby installed - ruby 2.2.4p230 (2015-12-16 revision 53155) [x86_64-linux]
rvm installed - rvm 1.27.0 (latest) by Wayne E. Seguin <wayneeseguin@gmail.com>, Michal Papis <mpapis@gmail.com> [https://rvm.io/]
bosh installed - BOSH 1.3184.1.0
bosh-init installed - version 0.0.81-775439c-2015-12-09T00:36:03Z
jq installed - jq-1.5
spruce installed - spruce - Version 1.7.0
safe installed - safe v0.0.23
vault installed - Vault v0.6.0
genesis installed - genesis 1.5.2 (61864a21370c)

git user.name  is 'J User'
git user.email is 'juser@starkandwayne.com'
```

## Proto Environment

![Global Network Diagram][global_network_diagram]

There are three layers to `genesis` templates.

* Global
* Site
* Environment

### Site Name

Sometimes the site level name can be a bit tricky because each IaaS divides things
differently.  With OpenStack we suggest a default of the OpenStack Datacenter you're using, for
example: `dc01`.

### Environment Name

All of the software the **proto-BOSH** will deploy will be in the `proto` environment.
And by this point, you've setup your credentials and used Terraform to construct
the IaaS components and configure your bastion host.  We're ready now to setup a BOSH
Director on the bastion.

The first step is to create a **vault-init** process.

### vault-init

![vault-init][bastion_1]

BOSH has secrets.  Lots of them.  Components like NATS and the database rely on
secure passwords for inter-component interaction.  Ideally, we'd have a spinning
Vault for storing our credentials, so that we don't have them on-disk or in a
git repository somewhere.

However, we are starting from almost nothing, so we don't have the luxury of
using a BOSH-deployed Vault.  What we can do, however, is spin a single-threaded
Vault server instance **on the bastion host**, and then migrate the credentials to
the real Vault later.

This we call a **vault-init**.  Because it precedes the **proto-BOSH** and Vault
deploy we'll be setting up later.

The `jumpbox` script that we ran as part of setting up the bastion host installs
the `vault` command-line utility, which includes not only the client for
interacting with Vault (`safe`), but also the Vault server daemon itself.

#### Start Server

Were going to start the server and do an overview of what the output means.  To
start the **vault-init**, run the `vault server` with the `-dev` flag.

**NOTE**: When you run the `vault server -dev` command, we recommend running it
in the foreground using either a `tmux` session or a separate ssh tab.  Also, we
do need to capture the output of the `Root Token`.

```
$ vault server -dev
==> WARNING: Dev mode is enabled!

In this mode, Vault is completely in-memory and unsealed.
Vault is configured to only have a single unseal key. The root
token has already been authenticated with the CLI, so you can
immediately begin using the Vault CLI.
```

A vault being unsealed sounds like a bad thing right?  But if you think about it
like at a bank, you can't get to what's in a vault unless it's unsealed.

And in dev mode, `vault server` gives the user the tools needed to authenticate.
We'll be using these soon when we log in.

```
The unseal key and root token are reproduced below in case you
want to seal/unseal the Vault or play with authentication.

Unseal Key:
781d77046dcbcf77d1423623550d28f152d9b419e09df0c66b553e1239843d89
Root Token: c888c5cd-bedd-d0e6-ae68-5bd2debee3b7
```

#### Setup vault-init

In order to setup the **vault-init** we need to target the server and authenticate.
We use `safe` as our CLI to do both commands.

The local `vault server` runs on `127.0.0.1` and on port `8200`.

```
$ safe target init http://127.0.0.1:8200
Now targeting init at http://127.0.0.1:8200

$ safe targets

  init  http://127.0.0.1:8200
```

Authenticate with the `Root Token` from the `vault server` output.

```
$ safe auth token
Authenticating against init at http://127.0.0.1:8200
Token: <paste your Root Token here>
```

#### Test vault-init

Here's a smoke test to see if you've setup the **vault-init** correctly.

```
$ safe set secret/handshake knock=knock
knock: knock

$ safe get secret/handshake
--- # secret/handshake
knock: knock
```

**NOTE**: If you receive `API 400 Bad Request` when attempting `safe set`, you may have incorrectly copied and entered your Root Key.  Try `safe auth token` again.

All set!  Now we can now build our deploy for the **proto-BOSH**.

### proto-BOSH

![proto-BOSH][bastion_2]

#### Generate BOSH Deploy

When using [the Genesis framework][genesis] to manage our deploys across
environments, each deployment will need its own repository. For each new deployment,
you'll need to change the remote URL in the `.git/config` file.

First setup a `ops` folder in your user's home directory.

```
$ mkdir -p ~/ops
$ cd ~/ops
```

Genesis has a template for BOSH deployments (including support for the
**proto-BOSH**), so let's use that by passing `bosh` into the `--template` flag.

```
$ genesis new deployment --template bosh
$ cd ~/ops/bosh-deployments
```

Next, we'll create a site and an environment from which to deploy our **proto-BOSH**.
The BOSH template comes with some site templates to help you get started
quickly, including:

- `aws` for Amazon Web Services VPC deployments
- `vsphere` for VMWare ESXi virtualization clusters
- `openstack` for OpenStack tenant deployments

When generating a new site we'll use this command format:

```
genesis new site --template <name> <site_name>
```

The template `<name>` will be `openstack` because that's our IaaS we're working with and
we recommend the `<site_name>` default to the OpenStack Datacenter, ex. `dc01`.

```
$ genesis new site --template openstack dc01
Created site dc01 (from template openstack):
~/ops/bosh-deployments/openstack
├── README
└── site
    ├── README
    ├── disk-pools.yml
    ├── jobs.yml
    ├── networks.yml
    ├── properties.yml
    ├── releases
    ├── resource-pools.yml
    ├── stemcell
    │   ├── name
    │   ├── sha1
    │   ├── url
    │   └── version
    └── update.yml

2 directories, 13 files
```

Finally, let's create our new environment, and name it `proto`
(that's `dc01/proto`, formally speaking).

```
$ genesis new env --type bosh-init dc01 proto
Running env setup hook: ~/ops/bosh-deployments/.env_hooks/setup

 init  http://127.0.0.1:8200

Use this Vault for storing deployment credentials?  [yes or no]
yes
Setting up credentials in vault, under secret/dc01/proto/bosh
.
└── secret/dc01/proto/bosh
    ├── blobstore/
    │   ├── agent
    │   └── director
    ├── db
    ├── nats
    ├── users/
    │   ├── admin
    │   └── hm
    └── vcap


Created environment dc01/:
~/ops/bosh-deployments/dc01/proto
├── credentials.yml
├── Makefile
├── name.yml
├── networking.yml
├── properties.yml
└── README

0 directories, 6 files
```

**NOTE** Don't forget that `--type bosh-init` flag is very important. Otherwise,
you'll run into problems with your deployment.

The template helpfully generated all new credentials for us and stored them in
our **vault-init**, under the `secret/dc01/proto/bosh` subtree.  Later, we'll
migrate this subtree over to our real Vault, once it is up and spinning.

#### Make Manifest

Let's head into the `proto/` environment directory and see if we
can create a manifest or (a more likely case) we still have to
provide some critical information:

```
$ cd ~/ops/bosh-deployments/dc01/proto
$ make manifest
9 error(s) detected:
 - $.cloud_provider.properties.openstack.default_key_name: What is your full key name?
 - $.cloud_provider.properties.openstack.default_security_groups: What Security Groups?
 - $.cloud_provider.ssh_tunnel.private_key: What is the local path to the Private Key for this deployment?  Due to a bug in Openstack Liberty and Mitaka, you need to use an SSH key generated by ssh-keygen, not one generated by Nova.
 - $.meta.openstack.api_key: Please supply an Openstack password
 - $.meta.openstack.auth_url: Please supply the authentication URL for the Openstack Identity Service
 - $.meta.openstack.tenant: Please supply an Openstack tenant name
 - $.meta.openstack.username: Please supply an Openstack user name
 - $.meta.shield_public_key: Specify the SSH public key from this environment's SHIELD daemon
 - $.networks.default.subnets: Specify subnets for your BOSH vm's network
Availability Zone will BOSH be in?


Failed to merge templates; bailing...
Makefile:22: recipe for target 'manifest' failed
make: *** [manifest] Error 5
```

Let's focus on the `$.meta` subtree, since that's where most parameters are defined in
Genesis templates:

```
- $.meta.openstack.api_key: Please supply an Openstack password
- $.meta.openstack.auth_url: Please supply the authentication URL for the Openstack Identity Service
- $.meta.openstack.tenant: Please supply an Openstack tenant name
- $.meta.openstack.username: Please supply an Openstack user name
```

This is easy enough to supply.  We'll put these properties in
`properties.yml`:

```
---
meta:
  openstack:
    api_key:  (( vault meta.vault_prefix "/openstack:api_key" ))
    tenant:   (( vault meta.vault_prefix "/openstack:tenant" ))
    username: (( vault meta.vault_prefix "/openstack:username" ))
    auth_url: http://identity.mydatacenter.io:5000/v2.0
    region: os-dc1
```

Configure the OpenStack credentials by pointing
Genesis to the Vault.  Let's go put those credentials in the
Vault:

```
$ export VAULT_PREFIX=secret/dc01/proto/os-dc1
$ safe set ${VAULT_PREFIX}/openstack tenant=cf username=cfadmin api_key=putyourpasswordhere
```

Let's try that `make manifest` again.

```
$ make manifest
5 error(s) detected:
- $.cloud_provider.properties.openstack.default_key_name: What is your full key name?
- $.cloud_provider.properties.openstack.default_security_groups: What Security Groups?
- $.cloud_provider.ssh_tunnel.private_key: What is the local path to the Private Key for this deployment?  Due to a bug in Openstack Liberty and Mitaka, you need to use an SSH key generated by ssh-keygen, not one generated by Nova.
- $.meta.shield_public_key: Specify the SSH public key from this environment's SHIELD daemon
- $.networks.default.subnets: Specify subnets for your BOSH vm's network


Failed to merge templates; bailing...
Makefile:22: recipe for target 'manifest' failed
make: *** [manifest] Error 5
```

Better. Let's configure our `cloud_provider` for OpenStack, using our OpenStack key pair.
We need copy our private key to the bastion host and path to the key for the
`private_key` entry in the following `properties.yml`.

On your local computer, you can copy to the clipboard with the `pbcopy` command
on a macOS machine:

```
cat ~/.ssh/bosh.pem | pbcopy
<paste values to /path/to/the/openstack/key.pem>
```

Then add the following to the `properties.yml` file.

```
---
meta:
...
cloud_provider:
  properties:
    openstack:
      default_key_name: bosh
      connection_options:
        connect_timeout: 600
      ignore_server_availability_zone: true
  ssh_tunnel:
    host: (( grab jobs.bosh.networks.default.static_ips.0 ))
    private_key: ~/.ssh/bosh
```

Note here the `ignore_server_availability_zone`.  This setting needs to be set to
`true` if the AZ for Cinder is not the same as the one for Nova.  Otherwise,
BOSH will have difficulty creating block storage volumes.

Once more, with feeling:

```
$ make manifest
3 error(s) detected:
 - $.cloud_provider.properties.openstack.default_security_groups: What Security Groups?
 - $.meta.shield_public_key: Specify the SSH public key from this environment's SHIELD daemon
 - $.networks.default.subnets: Specify subnets for your BOSH vm's network


Failed to merge templates; bailing...
Makefile:22: recipe for target 'manifest' failed
make: *** [manifest] Error 5
```

Excellent.  We're down to three issues.

We haven't deployed a SHIELD yet, so it may seem a bit odd that
we're being asked for an SSH public key.  When we deploy our
**proto-BOSH** via `bosh-init`, we're going to spend a fair chunk of
time compiling packages on the bastion host before we can actually
create and update the director VM.  `bosh-init` will delete the
director VM before it starts this compilation phase, so we will be
unable to do _anything_ while `bosh-init` is hard at work.  The
whole process takes about 30 minutes, so we want to minimize the
number of times we have to re-deploy **proto-BOSH**.  By specifying
the SHIELD agent configuration up-front, we skip a re-deploy after
SHIELD itself is up.

Let's leverage our Vault to create the SSH key pair for BOSH.
`safe` has a handy builtin for doing this:

```
$ safe ssh secret/dc01/proto/shield/keys/core
$ safe get secret/dc01/proto/shield/keys/core
--- # secret/dc01/proto/shield/keys/core
fingerprint: 40:9b:11:82:67:41:23:a8:c2:87:98:5d:ec:65:1d:30
private: |
  -----BEGIN RSA PRIVATE KEY-----
  MIIEowIBAAKCAQEA+hXpB5lmNgzn4Oaus8nHmyUWUmQFmyF2pa1++2WBINTIraF9
  ... etc ...
  5lm7mGwOCUP8F1cdPmpPNCkoQ/dx3T5mnsCGsb3a7FVBDDBje1hs
  -----END RSA PRIVATE KEY-----
public: |
  ssh-rsa AAAAB3NzaC...4vbnncAYZPTl4KOr
```

(output snipped for brevity and security; but mostly brevity)

Now we can put references to our Vaultified keypair in
`credentials.yml`:

```
---
meta:
  shield_public_key: (( vault "secret/dc01/proto/shield/keys/core:public" ))
```

You may want to take this opportunity to migrate
credentials-oriented keys from `properties.yml` into this file.

Now, we should have only two errors left when we `make
manifest`:

```
$ make manifest
2 error(s) detected:
 - $.cloud_provider.properties.openstack.default_security_groups: What Security Groups?
 - $.networks.default.subnets: Specify subnets for your BOSH vm's network


Failed to merge templates; bailing...
Makefile:22: recipe for target 'manifest' failed
make: *** [manifest] Error 5
```

So it's down to networking.

Refer back to your [Network Plan][netplan], and find the `global-infra-0`
subnet for the proto-BOSH in Horizon.  If you're using the plan in this
repository, that would be `10.4.1.0/24`, and we're allocating
`10.4.1.0/28` to our BOSH Director.  Our `networking.yml` file,
then, should look like this:

```
---
networks:
  - name: default
    subnets:
      - range: 10.4.1.0/24
        gateway: 10.4.1.1
        dns: [10.4.1.77, 10.4.1.78]
        cloud_properties:
          net_id: b5bfe2d1-fa17-41cc-9928-89013c27e266   # <- Global-Infra-0 Network UUID
        reserved:
          - 10.4.1.2 - 10.4.1.3
          - 10.4.1.10 - 10.4.1.254
        static:
          - 10.4.1.4
jobs:
  - name: bosh
    networks:
    - name: default
      static_ips: (( static_ips(0) ))

cloud_provider:
  properties:
    openstack:
      default_security_groups: [default]
```

Our range is that of the actual subnet we are in, `10.4.1.0/24`
(in reality, the `/28` allocation is merely a tool of bookkeeping).  As such, our
neutron-provided default gateway is 10.4.1.1 (the first available
IP from the associated router).  DNS needs to be the IP's provided by your
OpenStack administrator.

We identify our OpenStack-specific configuration under
`cloud_properties` by providing the **Network UUID**, NOT the subnet UUID, of
the internal neutron network we wish to use.  We also define the security groups
BOSH will be bound to.

Under the `reserved` block, we reserve the first few IPs (in case they are used
  for other network services such as DNS, etc.), and everything outside of
`10.4.1.0/28` (that is, `10.4.1.16` and above).

Finally, in `static` we reserve the first usable IP (`10.4.1.4`)
as static.  This will be assigned to our `bosh/0` director VM.

Now, `make manifest` should succeed (no output is a good sign),
and we should have a full manifest at `manifests/manifest.yml`:

```
$ make manifest
$ ls -l manifests/
total 8
-rw-r--r-- 1 ops staff 4572 Jun 28 14:24 manifest.yml
```

Now we are ready to deploy **proto-BOSH**.

```
$ make deploy
No existing genesis-created bosh-init statefile detected. Please
help genesis find it.
Path to existing bosh-init statefile (leave blank for new
deployments):
Deployment manifest: '~/ops/bosh-deployments/dc01/proto/manifests/.deploy.yml'
Deployment state: '~/ops/bosh-deployments/dc01/proto/manifests/.deploy-state.json'

Started validating
  Downloading release 'bosh'... Finished (00:00:09)
  Validating release 'bosh'... Finished (00:00:03)
  Downloading release 'bosh-openstack-cpi'... Finished (00:00:02)
  Validating release 'bosh-openstack-cpi'... Finished (00:00:00)
  Downloading release 'shield'... Finished (00:00:10)
  Validating release 'shield'... Finished (00:00:02)
  Validating cpi release... Finished (00:00:00)
  Validating deployment manifest... Finished (00:00:00)
  Downloading stemcell... Finished (00:00:01)
  Validating stemcell... Finished (00:00:00)
Finished validating (00:00:29)
...
```

(At this point, `bosh-init` starts the tedious process of
compiling all the things.  End-to-end, this is going to take about
a half an hour, so you probably want to go play [a game][slither]
or grab a cup of tea.)

...

All done?  Verify the deployment by trying to `bosh target` the
newly-deployed Director.  First you're going to need to get the
password out of our **vault-init**.

```
$ safe get secret/dc01/proto/bosh/users/admin
--- # secret/dc01/proto/bosh/users/admin
password: super-secret
```

Then, run target the director:

```
$ bosh target https://10.4.1.4:25555 proto-bosh
Target set to `dc01-proto-bosh'
Your username: admin
Enter password:
Logged in as `admin'

$ bosh status
Config
             ~/.bosh_config

Director
  Name       dc01-proto-bosh
  URL        https://10.4.1.4:25555
  Version    1.3232.2.0 (00000000)
  User       admin
  UUID       a43bfe93-d916-4164-9f51-c411ee2110b2
  CPI        openstack_cpi
  dns        disabled
  compiled_package_cache disabled
  snapshots  disabled

Deployment
  not set
```

All set!

Before you move onto the next step, you should commit your local
deployment files to version control, and push them up _somewhere_.
It's ok, thanks to Vault, Spruce and Genesis, there are no credentials or
anything sensitive in the template files.

### Generate Vault Deploy

We're building the infrastructure environment's vault.

![Vault][bastion_3]

Now that we have a **proto-BOSH** Director, we can use it to deploy
our real Vault.  We'll start with the Genesis template for Vault:

```
$ cd ~/ops
$ genesis new deployment --template vault
$ cd ~/ops/vault-deployments
```

**NOTE**: What is the "ops" environment? Short for operations, it's the
environment we're deploying the **proto-BOSH** and all the extra software that
monitors each of the child environments that will deployed later by the
**proto-BOSH** Director.

As before (and as will become almost second-nature soon), let's
create our `dc01` site using the `openstack` template, and then create
the `ops` environment inside of that site.

```
$ genesis new site --template openstack dc01
$ genesis new env dc01 proto
```

Answer yes twice and then enter a name for your Vault instance when prompted for a FQDN.

```
$ cd ~/ops/vault-deployments/dc01/proto
$ make manifest
7 error(s) detected:
- $.meta.openstack.azs.z1: Define the z1 OpenStack availability zone
- $.meta.openstack.azs.z2: Define the z2 OpenStack availability zone
- $.meta.openstack.azs.z3: Define the z3 OpenStack availability zone
- $.networks.vault_z1.subnets: Specify the z1 network for vault
- $.networks.vault_z2.subnets: Specify the z2 network for vault
- $.networks.vault_z3.subnets: Specify the z3 network for vault
- $.properties.vault.ha.domain: What fully-qualified domain name do you want to access your Vault at?


Failed to merge templates; bailing...
Makefile:22: recipe for target 'manifest' failed
make: *** [manifest] Error 5
```

Vault is pretty self-contained, and doesn't have any secrets of
its own.  All you have to supply is your network configuration,
and any IaaS settings.

Referring back to our [Network Plan][netplan] again, we
find that Vault should be striped across three zone-isolated
networks:

  - **10.4.1.16/28** in zone 1 (a)
  - **10.4.2.16/28** in zone 2 (b)
  - **10.4.3.16/28** in zone 3 (c)

First, lets do our OpenStack-specific region/zone configuration, along with our Vault HA fully-qualified domain name in `properties.yml`:

```
---
meta:
  openstack:
    azs:
      z1: dc01
      z2: dc01
      z3: dc01

properties:
  vault:
    ha:
      domain: 10.4.1.17
```

Our `/28` ranges are actually in their corresponding `/24` ranges
because the `/28`'s are (again) just for bookkeeping and ACL
simplification.  That leaves us with this for our
`networking.yml`:

```
---
networks:
  - name: vault_z1
    subnets:
    - range:    10.4.1.0/24
      gateway:  10.4.1.1
      dns:     [8.8.8.8, 8.8.4.4]
      cloud_properties:
        net_id: b5bfe2d1-fa17-41cc-9928-89013c27e266   #  ID for global-infra-0
        security_groups: [wide-open]
      reserved:
        - 10.4.1.2 - 10.4.1.15
        - 10.4.1.32 - 10.4.1.254                       # Vault (z1) is in 10.4.1.16/28
      static:
        - 10.4.1.16 - 10.4.1.18

  - name: vault_z2
    subnets:
    - range:    10.4.2.0/24
      gateway:  10.4.2.1
      dns:     [8.8.8.8, 8.8.4.4]
      cloud_properties:
        net_id: 2977ae9f-88f5-4d12-ad8e-1e393731ebb7   #  ID for global-infra-1
        security_groups: [wide-open]
      reserved:
        - 10.4.2.2 - 10.4.2.15
        - 10.4.2.32 - 10.4.2.254                       # Vault (z2) is in 10.4.2.16/28
      static:
        - 10.4.2.16 - 10.4.2.18

  - name: vault_z3
    subnets:
    - range:    10.4.3.0/24
      gateway:  10.4.3.1
      dns:     [8.8.8.8, 8.8.4.4]
      cloud_properties:
        net_id: 47f76643-ee72-44a3-b47f-a43e9c6ea8d2   #  ID for global-infra-2
        security_groups: [wide-open]
      reserved:
        - 10.4.3.2 - 10.4.3.15
        - 10.4.3.32 - 10.4.3.254                       # Vault (z3) is in 10.4.3.16/28
      static:
        - 10.4.3.16 - 10.4.3.18
```

That's a ton of configuration, but when you break it down it's not
all that bad.  We're defining three separate networks (one for
each of the three availability zones).  Each network has a unique
OpenStack Network UUID, but they share the same Security Groups, since
we want uniform access control across the board.

The most difficult part of this configuration is getting the
reserved ranges and static ranges correct, and self-consistent
with the network range / gateway / DNS settings.  This is a bit
easier since our network plan allocates a different `/24` to each
zone network, meaning that only the third octet has to change from
zone to zone (x.x.1.x for zone 1, x.x.2.x for zone 2, etc.)

Now, let's try a `make manifest` again (no output is a good sign):

```
$ make manifest
```

And then let's give the deploy a whirl:

```
$ make deploy
Acting as user 'admin' on 'dc01-proto-bosh'
Checking whether release consul/20 already exists...NO
Using remote release `https://bosh.io/d/github.com/cloudfoundry-community/consul-boshrelease?v=20'

Director task 1

```

Thanks to Genesis, we don't even have to upload the BOSH releases
(or stemcells) ourselves!

### Initializing Your Global Vault

Now that the Vault software is spinning, you're going to need to
initialize the Vault, which generates a root token for interacting
with the Vault, and a set of 5 _seal keys_ that will be used to
unseal the Vault so that you can interact with it.

First off, we need to find the IP addresses of our Vault nodes:

```
$ bosh vms dc01-proto-vault
+---------------------------------------------------+---------+-----+----------+-----------+
| VM                                                | State   | AZ  | VM Type  | IPs       |
+---------------------------------------------------+---------+-----+----------+-----------+
| vault_z1/0 (9fe19a85-e9ed-4bab-ac80-0d3034c5953c) | running | n/a | small_z1 | 10.4.1.16 |
| vault_z2/0 (13a46946-cd06-46e5-8672-89c40fd62e5f) | running | n/a | small_z2 | 10.4.2.16 |
| vault_z3/0 (3b234173-04d4-4bfb-b8bc-5966592549e9) | running | n/a | small_z3 | 10.4.3.16 |
+---------------------------------------------------+---------+-----+----------+-----------+
```

(Your UUIDs may vary, but the IPs should be close.)

Let's target the vault at 10.4.1.16:

```
$ export VAULT_ADDR=https://10.4.1.16:8200 && export VAULT_SKIP_VERIFY=1
```

We have to set `$VAULT_SKIP_VERIFY` to a non-empty value because we
used self-signed certificates when we deployed our Vault. The error message is as following if we did not do `export VAULT_SKIP_VERIFY=1`.

```
!! Get https://10.4.1.16:8200/v1/secret?list=1: x509: cannot validate certificate for 10.4.1.16 because it doesn't contain any IP SANs
```

Ideally, in a production-ready environment you will not be using self-signed
certificates. In that case, you would skip this step.

Let's initialize the Vault:

```
$ vault init
Unseal Key 1: c146f038e3e6017807d2643fa46d03dde98a2a2070d0fceaef8217c350e973bb01
Unseal Key 2: bae9c63fe2e137f41d1894d8f41a73fc768589ab1f210b1175967942e5e648bd02
Unseal Key 3: 9fd330a62f754d904014e0551ac9c4e4e520bac42297f7480c3d651ad8516da703
Unseal Key 4: 08e4416c82f935570d1ca8d1d289df93a6a1d77449289bac0fa9dc8d832c213904
Unseal Key 5: 2ddeb7f54f6d4f335010dc5c3c5a688b3504e41b749e67f57602c0d5be9b042305
Initial Root Token: e63da83f-c98a-064f-e4c0-cce3d2e77f97

Vault initialized with 5 keys and a key threshold of 3. Please
securely distribute the above keys. When the Vault is re-sealed,
restarted, or stopped, you must provide at least 3 of these keys
to unseal it again.

Vault does not store the master key. Without at least 3 keys,
your Vault will remain permanently sealed.
```

**Store these seal keys and the root token somewhere secure!!**
(A password manager like 1Password is an excellent option here.)

Unlike the dev-mode **vault-init** we spun up at the very outset,
this Vault comes up sealed, and needs to be unsealed using three
of the five keys above, so let's do that.

```
$ vault unseal
Key (will be hidden):
Sealed: true
Key Shares: 5
Key Threshold: 3
Unseal Progress: 1

$ vault unseal
...

$ vault unseal
Key (will be hidden):
Sealed: false
Key Shares: 5
Key Threshold: 3
Unseal Progress: 0
```

Now, let's switch back to using `safe`:

```
$ safe target https://10.4.1.16:8200 proto
Now targeting proto at https://10.4.1.16:8200

$ safe auth token
Authenticating against proto at https://10.4.1.16:8200
Token:

$ safe set secret/handshake knock=knock
knock: knock
```

### Migrating Credentials

You should now have two `safe` targets, one for first Vault
(named 'init') and another for the real Vault (named 'proto'):

```
$ safe targets

(*) proto     https://10.4.1.16:8200
    init      http://127.0.0.1:8200

```

Our `proto` Vault should be empty; we can verify that with `safe
tree`:

```
$ safe target proto -- tree
Now targeting proto at https://10.4.1.16:8200
.
└── secret
    └── handshake

```

`safe` supports a handy import/export feature that can be used to
move credentials securely between Vaults, without touching disk,
which is exactly what we need to migrate from our dev-Vault to
our real one:

```
$ safe target init -- export secret | \
  safe target proto -- import
Now targeting proto at https://10.4.1.16:8200
Now targeting init at http://127.0.0.1:8200
wrote secret/dc01/proto/bosh/blobstore/director
wrote secret/dc01/proto/bosh/db
wrote secret/dc01/proto/bosh/vcap
wrote secret/dc01/proto/vault/tls
wrote secret/dc01
wrote secret/dc01/proto/bosh/blobstore/agent
wrote secret/dc01/proto/bosh/registry
wrote secret/dc01/proto/bosh/users/admin
wrote secret/dc01/proto/bosh/users/hm
wrote secret/dc01/proto/shield/keys/core
wrote secret/handshake
wrote secret/dc01/proto/bosh/nats

$ safe target proto -- tree
Now targeting proto at https://10.4.1.16:8200
.
└── secret
    ├── handshake
    ├── dc01
    └── dc01/
        └── proto/
            ├── bosh/
            │   ├── blobstore/
            │   │   ├── agent
            │   │   └── director
            │   ├── db
            │   ├── nats
            │   ├── registry
            │   ├── users/
            │   │   ├── admin
            │   │   └── hm
            │   └── vcap
            ├── shield/
            │   └── keys/
            │       └── core
            └── vault/
                └── tls
```

Voila!  We now have all of our credentials in our real Vault, and
we can kill the **vault-init** server process!

```
$ sudo pkill vault
```

**NOTE:** Since the `init` Vault is no longer valid, you may want to delete its
configuration out of your `~/.saferc` file by removing the `init: http://127.0.0.1:8200`
line and its associated target.

## SHIELD

![Shield][bastion_4]

SHIELD is our backup solution.  We use it to configure and
schedule regular backups of data systems that are important to our
running operation, like the BOSH database, Concourse, and Cloud
Foundry.

### Setting up Object Storage For Backup Archives

To help keep things isolated, we're going to set up a brand new
user just for backup archive storage.  It's a good idea to
name this user something like `backup` or `shield-backup` so that
no one tries to re-purpose it later, and so that it doesn't get
deleted.

We also need to generate an S3 access key for this user and store those credentials
in the Vault:

```
$ openstack ec2 credentials create --user shield-backup --project cf
+------------+----------------------------------+
| Field      | Value                            |
+------------+----------------------------------+
| access     | 453389616a724f74b5ba0c9e6874f77d |
| project_id | 4116a3a098e64ff086b21ffba9dd2b2e |
| secret     | 64206456a18946f88399103be7dc6a8f |
| trust_id   | None                             |
| user_id    | 95aaf239306f45759d0adc7f4855c12d |
+------------+----------------------------------+

$ export VAULT_PREFIX=secret/dc01/proto/shield
$ safe set ${VAULT_PREFIX}/s3 access_key secret_key
access_key [hidden]:
access_key [confirm]:

secret_key [hidden]:
secret_key [confirm]:
```

You're also going to want to provision a dedicated bucket to
store archives in, and name it something descriptive, like
`codex-backups`.

### Deploying SHIELD

We'll start out with the Genesis template for SHIELD:

```
$ cd ~/ops
$ genesis new deployment --template shield
$ cd shield-deployments
```

Now we can set up our `dc01` site using the `openstack` template, with a
`proto` environment inside of it:

```
$ genesis new site --template openstack dc01
$ genesis new env dc01 proto
$ cd dc01/proto
```

Next, we `make manifest` and see what we need to fill in.
```
$ make manifest
3 error(s) detected:
 - $.meta.az: What availability zone is SHIELD deployed to?
 - $.networks.shield.subnets: Specify your shield subnet
 - $.properties.shield.daemon.ssh_private_key: Specify the SSH private key that the daemon will use to talk to the agents


Failed to merge templates; bailing...
Makefile:22: recipe for target 'manifest' failed
make: *** [manifest] Error 5
```

By now, this should be old hat.  According to the [Network
Plan][netplan], the SHIELD deployment belongs in the
**10.4.1.32/28** network, in dc01.  Let's put that
information into `properties.yml`:

```
---
meta:
  az: dc01
```

As we found with Vault, the `/28` range is actually in it's outer
`/24` range, since we're just using the `/28` subdivision for
convenience.

```
---
networks:
  - name: shield
    subnets:
    - range:    10.4.1.0/24
      gateway:  10.4.1.1
      dns:     [8.8.8.8, 8.8.4.4]
      cloud_properties:
        net_id: b5bfe2d1-fa17-41cc-9928-89013c27e266   #  ID for global-infra-0
        security_groups: [wide-open]
      reserved:
        - 10.4.1.2 - 10.4.1.31
        - 10.4.1.48 - 10.4.1.254                       # SHIELD is in 10.4.1.32/28
      static:
        - 10.4.1.32 - 10.4.1.32
  - name: floating
    type: vip
    cloud_properties:
      net_id: 09b03d93-45f8-4bea-b3b8-7ad9169f23d5     # ID for public
      security_groups: [wide-open]
jobs:
- name: shield
  networks:
  - name: shield
    default: [dns, gateway]
  - name: floating
    static_ips:
    - 192.168.10.121
```

(Don't forget to change your `subnet` to match your OpenStack Network UUID and
associated security group.)

Also, in this case (as will be seen later with things like Bolo and Cloud Foundry),
we are adding a Floating IP to this instance, so we can access the SHIELD UI.  To do
this, we have added another network called `floating`, gave it a type `vip`, and
gave it the Network UUID of the **public** network created near the beginning of
this walkthrough.  We also associate this floating IP by adding the `floating` network
directly to the `shield` job, as shown in the above manifest.

Then we need to configure our `store` and a default `schedule` and `retention` policy in `properties.yml`:

```
---
...

properties:
  shield:
    skip_ssl_verify: true
    store:
      name: "default"
      plugin: "s3"
      config:
        access_key_id: (( vault "secret/dc01:access_key" ))
        secret_access_key: (( vault "secret/dc01:secret_key" ))
        bucket: xxxxxx          # <- backup's s3 bucket
        prefix: "/"
    schedule:
      "default": "daily 3am"    # The key name provided is the name that will be used in the SHIELD UI
    retention:
      "default": "86400"        # 24 hours
```

Finally, if you recall, we already generated an SSH keypair for
SHIELD, so that we could pre-deploy the public key to our
**proto-BOSH**.  We stuck it in the Vault, at
`secret/dc01/proto/shield/keys/core`, so let's get it back out for this
deployment in `credentials.yml`:

```
---
properties:
  shield:
    daemon:
      ssh_private_key: (( vault meta.vault_prefix "/keys/core:private"))
```

Now, our `make manifest` should succeed (and not complain)

```
$ make manifest
```

Time to deploy!

```
$ make deploy
Acting as user 'admin' on 'dc01-proto-bosh'
Checking whether release shield/6.3.0 already exists...NO
Using remote release `https://bosh.io/d/github.com/starkandwayne/shield-boshrelease?v=6.3.0'

Director task 13
  Started downloading remote release > Downloading remote release

```

Once that's complete, you will be able to access your SHIELD
deployment, and start configuring your backup jobs.

### How to Use SHIELD

Backup jobs for SHIELD are created and maintained in the SHIELD UI:

![SHIELD UI][shield_ui]

To access the SHIELD UI, go to https://192.168.10.121.
The user name is `shield` and the password can be accessed in Vault by running
`safe get secret/dc01/proto/shield/webui:password`. We recommend also storing this
password in a password manager for convenience.

In the SHIELD deployment, we defined a default schedule and retention policy as
well as provided access to the blobstore. We can add additional policies in the
SHIELD UI.

To create a backup schedule, click on the **Schedules** tab and then **Create New
Schedule**. Since schedule times are given in UT, it is helpful to include your local
time in the schedule name and/or summary field. For example, if you were in Brisbane,
Australia you might name your backup job "Daily at 2 AM" and then provide the
schedule as "daily 4 pm" or "daily 16:00". You can even use "daily at 16:05" if
desired.

There are actually quite a few keywords available allowing you to create backups
that are `hourly`, `daily`, `weekly`, or `monthly` using those keywords. Here
are some additional backup schedules to show their behaviors: "hourly at 45 after",
"thursdays at 23:35", "3rd Tuesday at 2:05", and "monthly at 2:05 on 14th".  Once
you have provided the name, schedule, and optional summary click **Create** to finish.

Now that you have some additional backup schedules, we're going to create more
**retention policies** as well. Click on **Retention** and **Create New Retention Policy**.
Similar to the schedules, it is helpful to include the duration in the policy name.
The duration is given in days, so if you wanted to keep a given backup for a year
you'd use `365` and perhaps name the policy "1 year retention".

Something to consider: people usually like comparing "this time, last period" backups.
By that we mean "I wonder what X looked like this time last year" or "I wonder what last
Monday looked like", so you might want to consider making your 1 year backups actually
13 months or your weekly backups 8 days. (And so on.)

For the **storage** you _can_ create additional storage configurations in the SHIELD
UI by clicking on **Storage** and **Create New Store**. Currently SHIELD has an s3
plugin for storage, so your blobstore must be either S3 or S3-compatible. This said,
due to credential management, we strongly encourage you to put the storage configurations
in the SHIELD deployment itself and allow Vault to manage the credentials of the new
blobstores. To create a new store in the UI you will need to supply the configuration
as a JSON object, e.g.:

```
{
  "access_key_id": "ACCESS_KEY",
  "bucket": "192-168-10-154-sslip-io-shield",
  "prefix": "/DESIRED_PREFIX",
  "s3_host": "s3.192.168.10.154.sslip.io",
  "s3_port": "8080",
  "secret_access_key": "SECRET_ACCESS_KEY",
  "signature_version": "2",
  "skip_ssl_validation": true
}
```

Now that we have the where, when, and how long we need the "what". In SHIELD parlance
this is the **target**. In a minimal configuration, you'll want to back up the BOSH(es)
and Cloud Foundry(-ies). To add these as targets, go to **Targets** and **Create New Target**.

In the configuration we are using here, BOSH and Cloud Foundry are both using postgres
so the plugin name in this case will be "postgres". For BOSH the only database that
needs to be backed up is the bosh database, but for Cloud Foundry we'll need to back up
all of its databases.

To backup BOSH, use the postgres plugin and the BOSH director's IP and port `5444` for the
**Remote IP:Port**. The JSON configuration for a sample BOSH backup is:

```
{
  "pg_user": "boshdb",
  "pg_password": "",
  "pg_host": "127.0.0.1",
  "pg_port": "5432",
  "pg_bindir": "/var/vcap/packages/postgres-9.4/bin",
  "pg_dump_args": "",
  "pg_database": "bosh"
}
```

To backup Cloud Foundry, again use the postgres plugin, the IP address of the `postgres_z1` VM,
and port `5444`. The JSON configuration for a sample Cloud Foundry backup is:

```
{
  "pg_user": "vcap",
  "pg_password": "",
  "pg_host": "127.0.0.1",
  "pg_port": "5432",
  "pg_bindir": "/var/vcap/packages/postgres-9.4.9/bin",
  "pg_dump_args": ""
}
```

Notice that the only difference between these configurations is the `pg_database` field,
used in the BOSH case to back up only the BOSH database itself.

SHIELD currently has plugins for Redis, Mongo, Elasticsearch, and others. To see
more information about the plugin list and relevant documentation, please check out
the [SHIELD README][shield].

In order to back up BOSH, Cloud Foundry, and your services you will need to use the
schedules, retention policies, targets, and storage definitions and create a **backup job**.
To create the job, go to **Jobs** and **Create A New Job**. This is actually the easiest
part to configure - aside from providing the name and optional summary, everything else is a
drop down menu. This is where good naming really comes in handy! Once you have selected your
target, storage, schedule, and retention policy click **Create** to create the job.

In addition to running at the scheduled time, you can run a job at any time by clicking the
circular arrow icon for the desired job. Jobs can also be paused by clicking the adjacent
pause icon. This means that the job will not run at its scheduled time(s) until it is unpaused.

In order to **restore** a given backup, go to **Restore**. You can filter your backup jobs
by date and/or target name. The **Dashboard** gives a list of the most recent tasks and
their durations. Initially, most tasks are expected to have a very short duration but
as time goes on and your environment grows you will notice the time required for the various
backups will increase.

## bolo

![bolo][bastion_5]

Bolo is a monitoring system that collects metrics and state data
from your BOSH deployments, aggregates it, and provides data
visualization and notification primitives.

### Deploying Bolo Monitoring

You may opt to deploy Bolo once for all of your environments, in
which case it belongs in your management network, or you may
decide to deploy per-environment Bolo installations.  What you
choose mostly only affects your network topology / configuration.

To get started, you're going to need to create a Genesis
deployments repo for your Bolo deployments:

```
$ cd ~/ops
$ genesis new deployment --template bolo
$ cd bolo-deployments
```

Next, we'll create a site for your datacenter or VPC.  The bolo
template deployment offers some site templates to make getting
things stood up quick and easy, including:

- `aws` for Amazon Web Services VPC deployments
- `openstack` for OpenStack deployments
- `vsphere` for VMWare ESXi virtualization clusters
- `bosh-lite` for deploying and testing locally

```
$ genesis new site --template openstack dc01
Created site dc01 (from template openstack):
~/ops/bolo-deployments/dc01
├── README
└── site
    ├── disk-pools.yml
    ├── jobs.yml
    ├── networks.yml
    ├── properties.yml
    ├── releases
    ├── resource-pools.yml
    ├── stemcell
    │   ├── name
    │   └── version
    └── update.yml

2 directories, 10 files
```

Now, we can create our environment.

```
$ cd ~/ops/bolo-deployments/dc01
$ genesis new env dc01 proto
Created environment dc01/proto:
~/ops/bolo-deployments/dc01/proto
├── Makefile
├── README
├── cloudfoundry.yml
├── credentials.yml
├── director.yml
├── monitoring.yml
├── name.yml
├── networking.yml
├── properties.yml
└── scaling.yml

0 directories, 10 files
```

Bolo deployments have no secrets, so there isn't much in the way
of environment hooks for setting up credentials.

Now let's make the manifest.

```
$ cd ~/ops/bolo-deployments/dc01/proto
$ make manifest

2 error(s) detected:
 - $.meta.az: What availability zone is Bolo deployed to?
 - $.networks.bolo.subnets: Specify your bolo subnet

Failed to merge templates; bailing...
Makefile:22: recipe for target 'manifest' failed
make: *** [manifest] Error 5
```

From the error message, we need to configure the following things for an OpenStack deployment of
bolo:

- Availability Zone (via `meta.az`)
- Networking configuration

According to the [Network Plan][netplan], the bolo deployment belongs in the
**10.4.1.64/28** network, in dc01. Let's configure the availability zone in `properties.yml`:

```
---
meta:
  az: dc01
```

Since `10.4.1.64/28` is subdivision of the `10.4.1.0/24` subnet, we can configure networking as follows.
Once again, we add a Floating IP so we can access Gnossis.

```
---
networks:
  - name: bolo
    subnets:
    - range: 10.4.1.0/24
      gateway: 10.4.1.1
      cloud_properties:
        net_id: b5bfe2d1-fa17-41cc-9928-89013c27e266   #  ID for global-infra-0
        security_groups: [wide-open]
      dns: [8.8.8.8, 8.8.4.4]
      reserved:
        - 10.4.1.2   - 10.4.1.3
        - 10.4.1.4 - 10.4.1.63
         # Bolo is in 10.4.1.64/28
        - 10.4.1.80 - 10.4.1.254
      static:
        - 10.4.1.65 - 10.4.1.68
  - name: floating
    type: vip
    cloud_properties:
      net_id: 09b03d93-45f8-4bea-b3b8-7ad9169f23d5
      security_groups: [wide-open]

jobs:
- name: bolo
  networks:
  - name: floating
    static_ips:
    - 192.168.10.114
```

You can validate your manifest by running `make manifest` and
ensuring that you get no errors (no output is a good sign).

Then, you can deploy to your BOSH Director via `make deploy`.

Once you've deployed, you can validate the deployment via `bosh deployments`. You should see the bolo deployment. You can find the IP of bolo vm by running `bosh vms` for bolo deployment. In order to visit the [Gnossis](https://github.com/bolo/gnossis) web interface on your `bolo/0` VM from your browser on your laptop, you need to setup port forwarding to enable it.

One way of doing it is using ngrok, go to [ngrok Downloads] [ngrok-download] page and download the right version to your `bolo/0` VM, unzip it and run `./ngrok http 80`, it will output something like this:

```
ngrok by @inconshreveable                                                                                                                                                                   (Ctrl+C to quit)

Tunnel Status                 online
Version                       2.1.3
Region                        United States (us)
Web Interface                 http://127.0.0.1:4040
Forwarding                    http://18ce4bd7.ngrok.io -> localhost:80
Forwarding                    https://18ce4bd7.ngrok.io -> localhost:80

Connections                   ttl     opn     rt1     rt5     p50     p90
                              0       0       0.00    0.00    0.00    0.00
```

Copy the http or https link for forwarding and paste it into your browser, you
will be able to visit the Gnossis web interface for bolo.

If you do not want to use ngrok, you can simply use your local built-in SSH client as follows:

```
ssh bastion -L 4040:<ip address of your bolo server>:80 -N
```

Then, go to http://127.0.0.1:4040 in your web browser.

Out of the box, the Bolo installation will begin monitoring itself
for general host health (the `linux` collector), so you should
have graphs for bolo itself.

### Configuring Bolo Agents

Now that you have a Bolo installation, you're going to want to
configure your other deployments to use it.  To do that, you'll
need to add the `bolo` release to the deployment (if it isn't
already there), add the `dbolo` template to all the jobs you want
monitored, and configure `dbolo` to submit metrics to your
`bolo/0` VM in the bolo deployment.

**NOTE**: This may require configuration of network ACLs, security groups, etc.
If you experience issues with this step, you might want to start looking in
those areas first.

We will use shield as an example to show you how to configure Bolo Agents.

To add the release:

```
$ cd ~/ops/shield-deployments
$ genesis add release bolo latest
$ cd ~/ops/shield-deployments/dc01/proto
$ genesis use release bolo
```

If you do a `make refresh manifest` at this point, you should see a new
release being added to the top-level `releases` list.

To configure dbolo, you're going to want to add a line like the
last one here to all of your job template definitions:

```
jobs:
  - name: shield
    templates:
      - { release: bolo, name: dbolo }
```

Then, to configure `dbolo` to submit to your Bolo installation,
add the `dbolo.submission.address` property either globally or
per-job (strong recommendation for global, by the way).

If you have specific monitoring requirements, above and beyond
the stock host-health checks that the `linux` collector provides,
you can change per-job (or global) properties like the dbolo.collectors properties.

You can put those configuration in the `properties.yml` as follows:

```
properties:
  dbolo:
    submission:
      address: x.x.x.x # your Bolo VM IP
    collectors:
      - { every: 20s, run: 'linux' }
      - { every: 20s, run: 'httpd' }
      - { every: 20s, run: 'process -n nginx -m nginx' }
```

Remember that you will need to supply the `linux` collector
configuration, since Bolo skips the automatic `dbolo` settings you
get for free when you specify your own configuration.

### Further Reading on Bolo

More information can be found in the [Bolo BOSH Release README][bolo]
which contains a wealth of information about available graphs,
collectors, and deployment properties.

## Concourse

![Concourse][bastion_6]

### Deploying Concourse

Make sure you are targeting the proto vault:

```
$ safe target
Currently targeting proto at https://10.4.1.16:8200
```


From the `~/ops` folder let's generate a new `concourse` deployment, using the `--template` flag.

```
$ genesis new deployment --template concourse
```

Inside the `global` deployment level goes the site level definition.  For this concourse setup we'll use an `openstack` template for an `dc01` site.

```
$ genesis new site --template openstack dc01
Created site dc01 (from template openstack):
~/ops/concourse-deployments/dc01
├── README
└── site
    ├── disk-pools.yml
    ├── jobs.yml
    ├── networks.yml
    ├── properties.yml
    ├── releases
    ├── resource-pools.yml
    ├── stemcell
    │   ├── name
    │   └── version
    └── update.yml

2 directories, 10 files
```

Finally now, because our vault is setup and targeted correctly we can generate our `environment` level configurations.  And begin the process of setting up the specific parameters for our environment.

```
$ cd ~/ops/concourse-deployments
$ genesis new env dc01 proto
Running env setup hook: ~/ops/concourse-deployments/.env_hooks/00_confirm_vault

(*) proto   https://10.4.1.16:8200
    init    http://127.0.0.1:8200

Use this Vault for storing deployment credentials?  [yes or no] yes
Running env setup hook: ~/ops/concourse-deployments/.env_hooks/gen_creds
Generating credentials for Concourse CI
Created environment openstack/proto:
~/ops/concourse-deployments/dc01/proto
├── cloudfoundry.yml
├── credentials.yml
├── director.yml
├── Makefile
├── monitoring.yml
├── name.yml
├── networking.yml
├── properties.yml
├── README
└── scaling.yml

```

Let's make the manifest:

```
$ cd ~/ops/concourse-deployments/dc01/proto
$ make manifest
5 error(s) detected:
  - $.meta.availability_zone: What availability zone should your concourse VMs be in?
  - $.meta.external_url: What is the external URL for this concourse?
  - $.meta.shield_authorized_key: Specify the SSH public key from this environment's SHIELD daemon
  - $.meta.ssl_pem: Want ssl? define a pem
  - $.networks.concourse.subnets: Specify your concourse subnet

Failed to merge templates; bailing...
Makefile:22: recipe for target 'manifest' failed
make: *** [manifest] Error 5
```

Again starting with `meta` lines in `~/ops/concourse-deployments/dc01/proto/properties.yml`:

```
---
meta:
  availability_zone: "dc01"   # Set this to match your first zone
  external_url: "https://ci.192.168.10.115.sslip.io"  # Set as Floating IP address of the haproxy job
  ssl_pem: ~
  #  ssl_pem: (( vault meta.vault_prefix "/certs/haproxy:your_haproxy_domain" ))
  shield_authorized_key: (( vault "secret/dc01/proto/shield/keys/core:public" ))
```

The `~` means we won't use SSL certs for now.  If you have proper certs or want to use self signed you can add them to vault under the `web_ui:pem` key

For networking, we put this inside `proto` environment level (in `networking.yml`):

```
networks:
  - name: concourse
    subnets:
      - range: 10.4.1.0/24
        gateway: 10.4.1.1
        dns:     [8.8.8.8, 8.8.4.4]
        static:
          - 10.4.1.48 - 10.4.1.56  #Concourse uses 10.4.1.48/28
        reserved:
          - 10.4.1.2 - 10.4.1.3
          - 10.4.1.4 - 10.4.1.47
          - 10.4.1.65 - 10.4.1.254
        cloud_properties:
          net_id: b5bfe2d1-fa17-41cc-9928-89013c27e266
          security_groups: [wide-open]

  - name: floating
    type: vip
    cloud_properties:
      net_id: 09b03d93-45f8-4bea-b3b8-7ad9169f23d5
      security_groups: [wide-open]

jobs:
- name: haproxy
  networks:
  - name: concourse
    default: [dns, gateway]
  - name: floating
    static_ips:
    - 192.168.10.115
```

After it is deployed, you can do a quick test by hitting the HAProxy machine

```
$ bosh vms dc01-proto-concourse
Acting as user 'admin' on deployment 'dc01-proto-concourse' on 'dc01-proto-bosh'

Director task 43

Task 43 done

+--------------------------------------------------+---------+-----+---------+------------+
| VM                                               | State   | AZ  | VM Type | IPs        |
+--------------------------------------------------+---------+-----+---------+------------+
| db/0 (fdb7a556-e285-4cf0-8f35-e103b96eff46)      | running | n/a | db      | 10.4.1.61  |
| haproxy/0 (5318df47-b138-44d7-b3a9-8a2a12833919) | running | n/a | haproxy | 10.4.1.51  |
| web/0 (ecb71ebc-421d-4caa-86af-81985958578b)     | running | n/a | web     | 10.4.1.48  |
| worker/0 (c2c081e0-c1ef-4c28-8c7d-ff589d05a1aa)  | running | n/a | workers | 10.4.1.62  |
| worker/1 (12a4ae1f-02fc-4c3b-846b-ae232215c77c)  | running | n/a | workers | 10.4.1.57  |
| worker/2 (b323f3ba-ebe4-4576-ab89-1bce3bc97e65)  | running | n/a | workers | 10.4.1.58  |
+--------------------------------------------------+---------+-----+---------+------------+

VMs total: 6
```

Verify you get a response from the `haproxy` IP address:

```
$ curl -i 10.4.1.51
HTTP/1.1 200 OK
Date: Thu, 02 Jan 2017 04:50:05 GMT
Content-Type: text/html; charset=utf-8
Transfer-Encoding: chunked

<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Concourse</title>
```

You can then run on a your local machine

```
$ ssh -L 8080:10.4.1.51:80 user@ci.x.x.x.x.sslip.io -i path_to_your_private_key
```

and hit http://localhost:8080 to get the Concourse UI. Be sure to replace `user`
with the `jumpbox` username on the bastion host and x.x.x.x with the IP address
of the bastion host.

After the Concourse deployment is working, it is time to setup pipelines for deployments. Once a single pipeline is set up, other pipelines are set up using a similar workflow.

### Run Genesis CI Basics

Genesis CI requires us to have at least three deployed environments: alpha, beta and manual (/auto). Alpha is usually a bosh-lite environment. In this model, once the changes are applied to the alpha deployment and pass the tests, the changes will then be pushed further down the pipeline automatically to the beta deployment, which is usually in a sandbox/development environment. Once the tests pass the beta environment, we can choose whether changes either manually or automatically trigger your production deployment and tests. If manual, then human intervention is required to trigger the production deployment, by clicking the plus button in the Concourse UI. If automatic, the pipeline itself will deploy to production environment once beta deployment passes the tests.

In the deployments repo, run the following commands:

```
genesis ci alpha site/env
genesis ci beta  site/env
genesis ci manual(or auto) site/env
```
Then simply run `genesis ci check` to see if everything is setup correctly and use `genesis ci flow` to review that if the pipeline flows are correct.

For the pipelines, you may recall in the beginning of the document that the proto-BOSH
is responsible for deploying SHIELD, Vault, etc. as well as the other BOSHes. The
other BOSHes are used to test deployment upgrades for each environment until reaching
the final environment. In the current configuration upgrades are initially deployed
to BOSH Lite (`dc01-alpha-bosh-lite`) and then once they are passing are automatically
deployed to the staging (`dc01-staging-bosh`) environment:

![pipelines][pipelines]

Currently the pipelines are production-ready, meaning they only need the director
information if/when a production environment is added. Typically, we recommend
making production deployments manual to prevent unintended and unscheduled changes
to production.

For a manual deployment, simply click on `dc01-prod` and then the `+` in the upper
right corner:

![manual-deploy][manual_deploy]

By the necessity of its design, the BOSH pipeline differs from the other pipelines.
It will deploy to `dc01-alpha-bosh-lite` first and once that passes it will use `dc01-proto-openvdc`
to deploy `dc01-staging-bosh`:

![bosh-pipeline][bosh_pipeline]

Both `dc01-alpha-bosh-lite` and `dc01-proto-openvdc` will need to be manually
updated using either `make manifest deploy` or `make refresh manifest deploy` (`make refresh`
will update the deployment with site and/or global changes that have been added).


### Set Up Vault

#### Create read-only policies

To limit the Vault access given to the Concourse workers, read-only policies should be created for the pipelines. The number of policies created depends on the required level of isolation.  For lower security development environments, one policy for all the pipelines which can access all the credentials under `secret/*` can be configured.  For higher security production environments, one policy per pipeline can be set up so each pipeline only has access to its own deployment secrets. It is not good for the workers to authenticate with Vault using root, since the root has full access to all secrets in all backends.

In order to create a read-only policy, run `vault auth` to log into Vault as root. Run `vault policies` to show all the policies which are already defined. In this example, we will only create one `read-only` policy for all of the pipelines. Run `vault policy-write read-only acl.hcl` to create a read-only policy which has read-only rights to the `secret/*` path, where `acl.hcl` is configured as follows:

```
path "secret/*" {
  policy = "read"
}
```
To confirm, run `vault policies` to see the `read-only` policy we created. Save the token for this policy and you can `vault auth` to try it out later.

#### Set up AppID Authentication

NOTE: If the CLI to set up the app-id method does not work for you, please check if you are using the right Vault CLI version.

Let’s take a look what type of auth methods are enabled:
```
$ vault auth -methods
Path    Type   Default TTL  Max TTL  Description
token/  token  system       system   token based credentials
```

First, we need to enable the `app-id` auth method by running `vault auth-enable app-id`.

```
$ vault auth -methods
Path     Type    Default TTL  Max TTL  Description
app-id/  app-id  system       system
token/   token   system       system   token based credentials
```
Next we need to configure an `app-id` token and `user-id` token, by writing to the correct backend paths.

```
vault write auth/app-id/map/app-id/your_app_id \
              value=read-only\
              display_name="Deployments pipeline"

vault write auth/app-id/map/user-id/your_user_id \
        value=your_app_id \
        cidr_block= your_concourse_network_block
```
Keep in mind that `genesis` v1.6.0 has a default name for `app-id` and `user-id` for each deployment. Make sure you replace `your_app_id` and `your_user_id` using those default names accordingly.

In future, we will switch to AppRole from AppId when `genesis` is ready for AppRole. For more details, please visit:  https://www.vaultproject.io/docs/auth/app-id.html

### Use FLY to Configure Pipelines

The `fly` CLI is downloaded from the web UI page in your Concourse environment
its version that is in sync with the version of Concourse that is deployed.

Set the fly target as `concourse`. If we do not use `concourse` as target name, we will need to specify it in the `ci/settings.yml` file.

`fly -t concourse login -c concourse_url`

You will be prompted for the user and password. The user name is `concourse` and the password is saved in `secret/dc01/proto/concourse/web_ui` in vault.  If you do not specify a team name, the CLI will log you into the default `main` team.

In this case, we are using Basic Auth.  For details on how to set up oAuth for your team, see [Authentication Management for Teams](#authentication-management-for-teams)

To learn more about how to use the `fly` managing your pipelines, click [here][fly].

### Configure and Generate Pipelines

Run `genesis ci repipe`. You will see the following errors:

```
Testing Vault authentication by retrieving secret/handshake
Key                     Value
---                     -----
refresh_interval        2592000
knock                   knock

2 error(s) detected:
 - $.aliases.target: Please define aliases for your BOSH directors (uuid -> addr)
 - $.auth: Please define your BOSH directors in ci/boshes.yml (and remove this line)

6 error(s) detected:
 - $.meta.github.owner: Please specify the name of the user / organization that owns the Github repository (in ci/settings.yml)
 - $.meta.github.private_key: Please generate an SSH Deployment Key for this repo and specify it in ci/settings.yml
 - $.meta.github.repo: Please specify the name of the Github repository (in ci/settings.yml)
 - $.meta.name: Please name this deployment pipeline (in ci/settings.yml)
 - $.meta.slack.channel: Please specify the channel (#name) or user (@user) to send messages to (in ci/settings.yml)
 - $.meta.slack.webhook: Please provide a Slack Integration WebHook (in ci/settings.yml)
```
We can tackle all the errors by configuring two files: `ci/boshes.yml` and `ci/settings.yml`. Before that, lets add a deployment key to the repo and write it to Vault, since we need it to configure our pipeline.

To generate an SSH key pair, use the following commands to write it to Vault. In the git repo, add the public key to the deploy key.

```
safe write secret/dc01/proto/concourse/deployment_keys "private_key_name@private_key_file"
safe write secret/dc01/proto/concourse/deployment_keys "pub_key_name@pub_key_file"

```

Finally, configure the files we mentioned earlier:

```
boshes.yml

# The UUIDs and director URL's of all bosh directors in your pipeline go here.
# Since we are using proto-bosh to deploy bosh-lite, dev-bosh and prod-bosh, using bosh-lite to deploy a regular bosh for pipeline alpha environment purpose, when you setup pipeline for BOSH, you will need configure for all the BOSHes which are involved.

aliases:
  target:
    bosh_uuid: bosh_director_url
    bosh_uuid: bosh_director_url
    bosh_uuid: bosh_director_url

# if you are setting up pipeline for BOSH, you also need to configure BOSH deployment names and URLs
    bosh_deployment_name: bosh_director_url
    bosh_deployment_name: bosh_director_url
    bosh_deployment_name: bosh_director_url

auth:
  https://x.x.x.x:25555:
    username: admin
    password: (( vault "path to your bosh admin secret" ))
  https://x.x.x.x:25555:
    username: admin
    password: (( vault "path to your bosh admin secret" ))
  https://x.x.x.x:25555:
    username: admin
    password: (( vault "path to your bosh admin secret" ))

```

```
settings.yml

meta:
  name: your_pipeline_name
  env:
     VAULT_ADDR: YOUR_VAULT_ADDRESS
     VAULT_SKIP_VERIFY: 1

  github:
    owner: your_github_user_account
    repo: your_repo_name
    private_key: (( vault "your path to deploy key which you wrote to vault earlier" ))

  slack:
    webhook: (( vault "your path to webhook url" ))
    channel: '#your_channel name'

 # If you are setting up pipeline for BOSH, you need to configure pause_for_existing_bosh_tasks as true
  pause_for_existing_bosh_tasks: true

```
After `genesis ci repipe` succeeds, follow the instructions it prints out to unpause your pipeline.

Note: We should never modify `ci/pipeline.yml` directly. `genesis ci repipe` will take what are in `.ci.yml`,` ci/settings.yml` and ` ci/boshes.yml` to generate `ci/pipeline.yml`.

### Adding Smoke Tests to Pipeline

If the deployment you set up pipeline for has a smoke tests errand, you can add it to your existing pipeline pretty easily by following the instruction below:

* Run `genesis ci smoke-test your_smoke_tests_errand_name`
* Run `genesis ci repipe`
* Answer `y` when prompted to apply the configuration

Your pipeline configuration is now updated.

### How to Use Concourse UI

Visit [https://ci.192.168.10.115.sslip.io](https://ci.192.168.10.115.sslip.io) in your browser, and you will see a "no pipelines configured" message in the middle of your screen. Click the **login** button on the top right, choose the main team to login. The username and password is the same with what you used when you run `fly -t concourse login -c concourse_url`. (If needed, you can retrieve the password from vault with `safe get secret/dc01/proto/concourse/web_ui:password`.) After you login, you will see the pipelines listed on the left you already configured as the main team. You can click the pipeline name to look at the specific jobs in rectangle boxes of that pipeline.

Click each rectangle, you can see the builds, tasks and other details about the corresponding job. To trigger a job manually, you can click the plus button on the right corner for that job. We recommend that the jobs which deploy to the production environment should be manually triggered, and all other jobs can be triggered automatically when the changes are pushed to the git repository.

### Authentication Management for Teams

For the main team, we use Github oAuth instead of Basic Auth. It is also possible to set up authentication for additional teams. For more information, please refer to the [Concourse Authentication docs](https://concourse.ci/authentication.html).

## Building out Sites and Environments

Now that the underlying infrastructure has been deployed, we can start deploying our alpha/beta/other sites, with Cloud Foundry, and any required services. When using Concourse to update BOSH deployments,
there are the concepts of `alpha` and `beta` sites. The alpha site is the initial place where all deployment changes are checked for sanity + deployability. Typically this is done with a `bosh-lite` VM. The `beta` sites are where site-level changes are vetted. Usually these are referred to as the sandbox or staging environments, and there will be one per site, by necessity. Once changes have passed both the alpha, and beta site, we know it is reasonable for them to be rolled out to other sites, like production.

### Alpha

#### BOSH-Lite

Since our `alpha` site will be a bosh lite running on OpenStack, we will need to deploy that to our [global infrastructure network][netplan].

First, lets make sure we're in the right place, targeting the right Vault:

```
$ cd ~/ops
$ safe target proto
Now targeting proto at https://10.4.1.16:8200
```

Now we can create our repo for deploying the bosh-lite:

```
$ genesis new deployment --template bosh-lite
cloning from template https://github.com/starkandwayne/bosh-lite-deployment
Cloning into '~/ops/bosh-lite-deployments'...
remote: Counting objects: 55, done.
remote: Compressing objects: 100% (33/33), done.
remote: Total 55 (delta 7), reused 55 (delta 7), pack-reused 0
Unpacking objects: 100% (55/55), done.
Checking connectivity... done.
Embedding genesis script into repository
genesis v1.5.2 (ec9c868f8e62)
[master 5421665] Initial clone of templated bosh-lite deployment
 3 files changed, 3672 insertions(+), 67 deletions(-)
  rewrite README.md (96%)
   create mode 100755 bin/genesis
```

Next lets create our site and environment:

```
$ cd bosh-lite-deployments
$ genesis new site --template openstack dc01
Created site dc01 (from template openstack):
~/ops/bosh-lite-deployments/dc01
├── README
└── site
    ├── disk-pools.yml
    ├── jobs.yml
    ├── networks.yml
    ├── properties.yml
    ├── README
    ├── releases
    ├── resource-pools.yml
    ├── stemcell
    │   ├── name
    │   └── version
    └── update.yml

2 directories, 11 files

$ genesis new env dc01 alpha
Running env setup hook: ~/ops/bosh-lite-deployments/.env_hooks/setup

(*) proto	https://10.4.1.16:8200

Use this Vault for storing deployment credentials?  [yes or no]yes
Setting up credentials in vault, under secret/dc01/alpha/bosh-lite
.
└── secret/dc01/alpha/bosh-lite
    ├── blobstore/


    │   ├── agent
    │   └── director
    ├── db
    ├── nats
    ├── users/
    │   ├── admin
    │   └── hm
    └── vcap




Created environment dc01/alpha:
~/ops/bosh-lite-deployments/dc01/alpha
├── cloudfoundry.yml
├── credentials.yml
├── director.yml
├── Makefile


├── monitoring.yml
├── name.yml
├── networking.yml
├── properties.yml
├── README
└── scaling.yml

0 directories, 10 files

```

Now lets try to deploy:

```
$ cd dc01/alpha/
$ make deploy
  checking https://genesis.starkandwayne.com for details on latest stemcell bosh-openstack-kvm-ubuntu-trusty-go_agent
  checking https://genesis.starkandwayne.com for details on release bosh/260
  checking https://genesis.starkandwayne.com for details on release bosh-warden-cpi/29
  checking https://genesis.starkandwayne.com for details on release garden-linux/0.342.0
  checking https://genesis.starkandwayne.com for details on release port-forwarding/6
  3 error(s) detected:
   - $.meta.openstack.azs.z1: What Availability Zone will BOSH be in?
   - $.meta.port_forwarding_rules: Define any port forwarding rules you wish to enable on the bosh-lite, or an empty array
   - $.networks.default.subnets: Specify subnets for your BOSH vm's network


Failed to merge templates; bailing...


Makefile:25: recipe for target 'deploy' failed
make: *** [deploy] Error 3
```

Looks like we only have a handful of parameters to update, all related to
networking, so lets fill out our `networking.yml`, after consulting the
[Network Plan][netplan] to find our global infrastructure network and Horizon
to find our Network UUID:

`networking.yml`

```
---
networks:
- name: default
  subnets:
  - cloud_properties:
      net_id: b5bfe2d1-fa17-41cc-9928-89013c27e266   #  ID for global-infra-0
      security_groups: [wide-open]
    dns:     [8.8.8.8]
    gateway: 10.4.1.1
    range:   10.4.1.0/24
```

Since there are a bunch of other deployments on the infrastructure network, we should take care
to reserve the correct static + reserved IPs, so that we don't conflict with other deployments. Fortunately
that data can be referenced in the [Global Infrastructure IP Allocation section][infra-ips] of the Network Plan:

`networking.yml`

```
---
networks:
- name: default
  subnets:
  - cloud_properties:
      net_id: b5bfe2d1-fa17-41cc-9928-89013c27e266   #  ID for global-infra-0
      security_groups: [wide-open]
    dns:     [8.8.8.8]
    gateway: 10.4.1.1
    range:   10.4.1.0/24
    reserved:
      - 10.4.1.2 - 10.4.1.79
      - 10.4.1.96 - 10.4.1.255
    static:
      - 10.4.1.80
```

As before, we will add the Floating IP so we can access Cloud Foundry when it is deployed to bosh-lite:

```
networks:
- name: default
  subnets:
  - cloud_properties:
      net_id: b5bfe2d1-fa17-41cc-9928-89013c27e266   #  ID for global-infra-0
      security_groups: [wide-open]
    dns:     [8.8.8.8]
    gateway: 10.4.1.1
    range:   10.4.1.0/24
    reserved:
      - 10.4.1.2 - 10.4.1.79
      - 10.4.1.96 - 10.4.1.255
    static:
      - 10.4.1.80
- name: floating
  type: vip
  cloud_properties:
    net_id: 09b03d93-45f8-4bea-b3b8-7ad9169f23d5
    security_groups: [wide-open]
jobs:
- name: bosh
  networks:
  - name: default
    default: [gateway, dns]
  - name: floating
    static_ips:
    - 192.168.10.124
```

Lastly, we will need to add port-forwarding rules, so that things outside the bosh-lite can talk to its services.
Since we know we will be deploying Cloud Foundry, let's add rules for it:

`properties.yml`
```
---
meta:
  openstack:
    azs:
      z1: dc01
  port_forwarding_rules:
    - internal_ip:   10.244.0.34
      internal_port: 80
      external_port: 80
    - internal_ip:   10.244.0.34
      internal_port: 443
      external_port: 443
```

And finally, we can deploy again:

```
$ make deploy
  checking https://genesis.starkandwayne.com for details on stemcell bosh-aws-xen-hvm-ubuntu-trusty-go_agent/3262.2
    checking https://genesis.starkandwayne.com for details on release bosh/256.2
  checking https://genesis.starkandwayne.com for details on release bosh-warden-cpi/29
    checking https://genesis.starkandwayne.com for details on release garden-linux/0.339.0
  checking https://genesis.starkandwayne.com for details on release port-forwarding/2
    checking https://genesis.starkandwayne.com for details on stemcell bosh-aws-xen-hvm-ubuntu-trusty-go_agent/3262.2
  checking https://genesis.starkandwayne.com for details on release bosh/256.2
    checking https://genesis.starkandwayne.com for details on release bosh-warden-cpi/29
  checking https://genesis.starkandwayne.com for details on release garden-linux/0.339.0
    checking https://genesis.starkandwayne.com for details on release port-forwarding/2
Acting as user 'admin' on 'dc01-proto-bosh'
Checking whether release bosh/256.2 already exists...YES
Acting as user 'admin' on 'dc01-proto-bosh'
Checking whether release bosh-warden-cpi/29 already exists...YES
Acting as user 'admin' on 'dc01-proto-bosh'
Checking whether release garden-linux/0.339.0 already exists...YES
Acting as user 'admin' on 'dc01-proto-bosh'
Checking whether release port-forwarding/2 already exists...YES
Acting as user 'admin' on 'dc01-proto-bosh'
Checking if stemcell already exists...
Yes
Acting as user 'admin' on deployment 'dc01-alpha-bosh-lite' on 'dc01-proto-bosh'
Getting deployment properties from director...
Unable to get properties list from director, trying without it...

Detecting deployment changes
...
Deploying
---------
Are you sure you want to deploy? (type 'yes' to continue): yes

Director task 58
  Started preparing deployment > Preparing deployment. Done (00:00:00)
...
Task 58 done

Started		2017-01-02 19:14:31 UTC
Finished	2017-01-02 19:17:42 UTC
Duration	00:03:11

Deployed `dc01-alpha-bosh-lite' to `dc01-proto-bosh'
```

Now we can verify the deployment and set up our `bosh` CLI target:

```
# grab the admin password for the bosh-lite
$ safe get secret/dc01/alpha/bosh-lite/users/admin
--- # secret/dc01/alpha/bosh-lite/users/admin
password: YOUR-PASSWORD-WILL-BE-HERE


$ bosh target https://10.4.1.80:25555 alpha
Target set to `dc01-alpha-bosh-lite'
Your username: admin
Enter password:
Logged in as `admin'
$ bosh status
Config
             ~/.bosh_config

 Director
   Name       dc01-alpha-bosh-lite
     URL        https://10.4.1.80:25555
   Version    1.3232.2.0 (00000000)
     User       admin
   UUID       d0a12392-f1df-4394-99d1-2c6ce376f821
     CPI        vsphere_cpi
   dns        disabled
     compiled_package_cache disabled
   snapshots  disabled

   Deployment
     not set
```

Tadaaa! Time to commit all the changes to deployment repo, and push to where we're storing
them long-term.

#### Alpha Cloud Foundry

To deploy CF to our alpha environment, we will need to first ensure we're targeting the right
Vault/BOSH:

```
$ cd ~/ops
$ safe target proto

(*) proto	https://10.4.1.16:8200

$ bosh target alpha
Target set to `dc01-alpha-bosh-lite'
```

Now we'll create our deployment repo for cloudfoundry:

```
$ genesis new deployment --template cf
cloning from template https://github.com/starkandwayne/cf-deployment
Cloning into '~/ops/cf-deployments'...
remote: Counting objects: 268, done.
remote: Compressing objects: 100% (3/3), done.
remote: Total 268 (delta 0), reused 0 (delta 0), pack-reused 265
Receiving objects: 100% (268/268), 51.57 KiB | 0 bytes/s, done.
Resolving deltas: 100% (112/112), done.
Checking connectivity... done.
Embedding genesis script into repository
genesis v1.5.2 (ec9c868f8e62)
[master 1f0c534] Initial clone of templated cf deployment
 2 files changed, 3666 insertions(+), 150 deletions(-)
 rewrite README.md (99%)
 create mode 100755 bin/genesis
```

And generate our bosh-lite based alpha environment:

```
$ cd cf-deployments
$ genesis new site --template bosh-lite bosh-lite
Created site bosh-lite (from template bosh-lite):
~/ops/cf-deployments/bosh-lite
├── README
└── site
    ├── disk-pools.yml
    ├── jobs.yml
    ├── networks.yml
    ├── properties.yml
    ├── releases
    ├── resource-pools.yml
    ├── stemcell
    │   ├── name
    │   └── version
    └── update.yml

2 directories, 10 files

$ genesis new env bosh-lite alpha
Running env setup hook: ~/ops/cf-deployments/.env_hooks/00_confirm_vault

(*) proto	https://10.4.1.16:8200

Use this Vault for storing deployment credentials?  [yes or no] yes
Running env setup hook: ~/ops/cf-deployments/.env_hooks/setup_certs
Generating Cloud Foundry internal certs
Uploading Cloud Foundry internal certs to Vault
Running env setup hook: ~/ops/cf-deployments/.env_hooks/setup_cf_secrets
Creating JWT Signing Key
Creating app_ssh host key fingerprint
Generating secrets
Created environment bosh-lite/alpha:
~/ops/cf-deployments/bosh-lite/alpha
├── cloudfoundry.yml
├── credentials.yml
├── director.yml
├── Makefile
├── monitoring.yml
├── name.yml
├── networking.yml
├── properties.yml
├── README
└── scaling.yml

0 directories, 10 files


```

Unlike all the other deployments so far, we won't use `make manifest` to vet the manifest for CF. This is because the bosh-lite CF comes out of the box ready to deploy to a Vagrant-based bosh-lite with no tweaks.  Since we are using it as the Cloud Foundry for our alpha environment, we will need to customize the Cloud Foundry base domain, with a domain resolving to the IP of our `alpha` bosh-lite VM:

```
cd bosh-lite/alpha
```
In `properties.yml`:

```
---
meta:
  cf:
    base_domain: 10.4.1.80.sslip.io
```

Now we can deploy:

```
$ make deploy
  checking https://genesis.starkandwayne.com for details on release cf/250
  checking https://genesis.starkandwayne.com for details on release toolbelt/3.3.0
  checking https://genesis.starkandwayne.com for details on release postgres/1.0.3
  ...
Acting as user 'admin' on 'dc01-try-anything-bosh-lite'
Checking whether release cf/250 already exists...NO
Using remote release `https://bosh.io/d/github.com/cloudfoundry/cf-release?v=250'

Director task 1
  Started downloading remote release > Downloading remote release
...
Deploying
---------
Are you sure you want to deploy? (type 'yes' to continue): yes

Director task 12
  Started preparing deployment > Preparing deployment. Done (00:00:01)
...
Task 12 done

Started		2017-01-02 14:47:45 UTC
Finished	2017-01-02 14:51:28 UTC
Duration	00:03:43

Deployed `bosh-lite-alpha-cf' to `dc01-try-anything-bosh-lite'
```

And once complete, run the smoke tests for good measure:

```
$ genesis bosh run errand smoke_tests
Acting as user 'admin' on deployment 'bosh-lite-alpha-cf' on 'dc01-alpha-bosh-lite'

Director task 18
  Started preparing deployment > Preparing deployment. Done (00:00:02)

  Started preparing package compilation > Finding packages to compile. Done (00:00:01)

  Started creating missing vms > smoke_tests/0 (c609e4c5-29e7-4f66-81e1-b94b9139ee7d). Done (00:00:08)

  Started updating job smoke_tests > smoke_tests/0 (c609e4c5-29e7-4f66-81e1-b94b9139ee7d) (canary). Done (00:00:23)

  Started running errand > smoke_tests/0. Done (00:02:18)

  Started fetching logs for smoke_tests/c609e4c5-29e7-4f66-81e1-b94b9139ee7d (0) > Finding and packing log files. Done (00:00:01)

  Started deleting errand instances smoke_tests > smoke_tests/0 (c609e4c5-29e7-4f66-81e1-b94b9139ee7d). Done (00:00:03)

Task 18 done

Started         2017-01-02 14:15:16 UTC
Finished        2017-01-02 14:18:12 UTC
Duration        00:02:56

[stdout]
################################################################################################################
go version go1.6.3 linux/amd64
CONFIG=/var/vcap/jobs/smoke-tests/bin/config.json
...

Errand 'smoke_tests' completed successfully (exit code 0)
```

We now have our alpha-environment's Cloud Foundry stood up!

#### Alpha haproxy


Haproxy primarily provides SSL termination for Cloud Foundry. It is also configured to provide SSL termination for S3 since the current Openstack environment doesn't have an SSL endpoint for S3, which is needed for SHIELD. This will not be needed for environments that have SSL endpoints for S3, e.g. production.

Since haproxy has a cert and credentials that need to go in Vault, make sure you are targeting the desired Vault:

```
$ safe target proto
```
Once that is taken care of, you will create the new deployment with its site and environment:

```
$ genesis new deployment --template haproxy
$ cd haproxy-deployments
$ genesis new site --template bosh-lite bosh-lite
$ genesis new env bosh-lite alpha
$ cd bosh-lite/alpha
$ make manifest
Found stemcell bosh-warden-boshlite-ubuntu-trusty-go_agent 3312.15 on director
Found release haproxy latest on director
release toolbelt track is set to track from the index
  checking https://genesis.starkandwayne.com for details on latest release toolbelt
2 error(s) detected:
 - $.properties.ha_proxy.backend_servers: Specify your go routers IPs as backend servers
 - $.properties.ha_proxy.tcp.cf_app_ssh.backend_servers: Specify you  CF Access VMs IPs as your backend servers


Failed to merge templates; bailing...
Makefile:22: recipe for target 'manifest' failed
make: *** [manifest] Error 5
```

As before, we'll resolve the errors by adding in the requested information, this time in the `properties.yml` file:

```
---
properties:
  ha_proxy:
    backend_servers: [ 10.244.0.22 ]       # Alpha Cloud Foundry router_z1 IP
    ssl_pem:
      - (( vault meta.vault_prefix "/certs/pems:*.192.168.10.124.sslip.io" ))

    tcp:
      - name: cf_app_ssh
        backend_servers: [ 10.244.0.109 ]  # Alpha Cloud Foundry access_z1 IP
```

You'll notice that we also added another field, `ssl_pem`. This stores the SSL cert that will be used by haproxy. In beta environments if you are using an `sslip.io` domain you will need to supply a floating IP to generate the cert, but since this is BOSH Lite we will simply supply the static IP assigned to the haproxy instance. To generate the cert, run the `haproxy_cert_gen` script in the `bin` directory:
```
$ ENV_PATH=secret/bosh-lite/alpha FIP=192.168.10.124 ./bin/haproxy_cert_gen
```

The script uses Cloud Foundry's CA, so the `ENV_PATH` supplied is the `vault_prefix` given in `name.yml` without the actual deployment name.

If you are not using an `sslip.io` domain and are using a domain with its own CA, you will not need to run this script. Rather you would use your own internal process to generate the certs and then add them to Vault with `safe write`.

Once this is done all the errors are resolved and you can run `make manifest deploy` to deploy haproxy.

### First Beta Environment

Now that our `alpha` environment has been deployed, we can deploy our first beta environment to OpenStack. To do this, we will first deploy a BOSH Director for the environment using the `bosh-deployments` repo we generated back when we built our [proto-BOSH](#proto-bosh), and then deploy Cloud Foundry on top of it.

#### BOSH
```
$ cd ~/ops/bosh-deployments
$ bosh target proto-bosh
$ ls
dc01  bin  global  LICENSE  README.md
```

We already have the `dc01` site created, so now we will just need to create our new environment, and deploy it. Different names (sandbox or staging) for Beta have been used for different customers, here we call it staging.


```
$ safe target proto
Now targeting proto at http://10.10.10.6:8200
$ genesis new env dc01 staging
RSA 1024 bit CA certificates are loaded due to old openssl compatibility
Running env setup hook: ~/ops/bosh-deployments/.env_hooks/setup

 proto	http://10.10.10.6:8200

Use this Vault for storing deployment credentials?  [yes or no] yes
Setting up credentials in vault, under secret/dc01/staging/bosh
.
└── secret/dc01/staging/bosh
    ├── blobstore/
    │   ├── agent
    │   └── director
    ├── db
    ├── nats
    ├── users/
    │   ├── admin
    │   └── hm
    └── vcap


Created environment dc01/staging:
~/ops/bosh-deployments/dc01/staging
├── cloudfoundry.yml
├── credentials.yml
├── director.yml
├── Makefile
├── monitoring.yml
├── name.yml
├── networking.yml
├── properties.yml
├── README
└── scaling.yml

0 directories, 10 files

```

Notice, unlike the **proto-BOSH** setup, we do not specify `--type bosh-init`. This means we will use BOSH itself (in this case the **proto-BOSH**) to deploy our sandbox BOSH. Again, the environment hook created all of our credentials for us, but this time we targeted the long-term Vault, so there will be no need for migrating credentials around.

Let's try to deploy now, and see what information still needs to be resolved:

```
$ make manifest

9 error(s) detected:
 - $.cloud_provider.properties.openstack.default_key_name: What is your full key name?
 - $.cloud_provider.properties.openstack.default_security_groups: What Security Groups?
 - $.cloud_provider.ssh_tunnel.private_key: What is the local path to the Private Key for this deployment?  Due to a bug in Openstack Liberty and Mitaka, you need to use an SSH key generated by ssh-keygen, not one generated by Nova.
 - $.meta.openstack.api_key: Please supply an Openstack password
 - $.meta.openstack.auth_url: Please supply the authentication URL for the Openstack Identity Service
 - $.meta.openstack.tenant: Please supply an Openstack tenant name
 - $.meta.openstack.username: Please supply an Openstack user name
 - $.meta.shield_public_key: Specify the SSH public key from this environment's SHIELD daemon
 - $.networks.default.subnets: Specify subnets for your BOSH vm's network


Failed to merge templates; bailing...
Makefile:22: recipe for target 'manifest' failed
make: *** [manifest] Error 5
```

Looks like we need to provide the same type of data as we did for **proto-BOSH**. Lets fill in the basic properties:

```
`properties.yml`

---
meta:
  openstack:
    api_key:  (( vault meta.vault_prefix "/openstack:api_key" ))
    tenant:   (( vault meta.vault_prefix "/openstack:tenant" ))
    username: (( vault meta.vault_prefix "/openstack:username" ))
    auth_url: http://identity.mydatacenter.io:5000/v2.0
    region: os-dc1
cloud_provider:
  properties:
    openstack:
      default_key_name: bosh
      connection_options:
        connect_timeout: 600
      ignore_server_availability_zone: true
  ssh_tunnel:
    host: (( grab jobs.bosh.networks.default.static_ips.0 ))
    private_key: ~/.ssh/bosh
properties:
  bolo:
    submission:
      address: 10.4.1.65
    collectors:
      - { every: 20s, run: 'linux' }
EOF
```

This was a bit easier than it was for **proto-BOSH** since our keys are already in Vault.

Verifying our changes worked, we see that we only need to provide networking configuration at this point:

```
make deploy
$ make deploy
1 error(s) detected:
 - $.networks.default.subnets: Specify subnets for your BOSH vm's network


Failed to merge templates; bailing...
make: *** [deploy] Error 3

```

All that remains is filling in our networking details, so lets go consult our [Network Plan](https://github.com/starkandwayne/codex/blob/master/network.md). We will place the BOSH Director in the staging site's infrastructure network, in the first AZ we have defined (subnet name `staging-infra-0`, CIDR `10.4.32.0/24`). To do that, we'll need to update `networking.yml`:

`networking.yml`:

```
---
networks:
  - name: default
    subnets:
      - range: 10.4.16.0/24
        gateway: 10.4.16.1
        dns: [10.4.1.77, 10.4.1.78]
        cloud_properties:
          net_id: 20c35573-3a0c-4725-95a2-b58550407fcf # <- Global-Infra-0 Network UUID
        reserved:
          - 10.4.16.2 - 10.4.16.3
          - 10.4.16.10 - 10.4.16.254
        static:
          - 10.4.16.4
  - name: floating
    type: vip
    cloud_properties:
      net_id: 09b03d93-45f8-4bea-b3b8-7ad9169f23d5
      security_groups: [wide-open]

jobs:
  - name: bosh
    networks:
    - name: default
      static_ips: (( static_ips(0) ))
    - name: floating
      static_ips:
      - 192.168.10.122

cloud_provider:
  properties:
    openstack:
      default_security_groups: [default]
EOF
```

Now that that's handled, let's deploy for real:

```
$ make deploy
RSA 1024 bit CA certificates are loaded due to old openssl compatibility
Acting as user 'admin' on 'openstack-proto-bosh-microboshen-openstack'
Checking whether release bosh/256.2 already exists...YES
Acting as user 'admin' on 'openstack-proto-bosh-microboshen-openstack'
Checking whether release bosh-openstack-cpi/53 already exists...YES
Acting as user 'admin' on 'openstack-proto-bosh-microboshen-openstack'
Checking whether release shield/6.2.1 already exists...YES
Acting as user 'admin' on 'openstack-proto-bosh-microboshen-openstack'
Checking if stemcell already exists...
Yes
Acting as user 'admin' on deployment 'dc01-staging-bosh' on 'openstack-proto-bosh-microboshen-openstack'
Getting deployment properties from director...

Detecting deployment changes
----------------------------
resource_pools:
- cloud_properties:
    availability_zone: us-east-1b
    ephemeral_disk:
      size: 25000
      type: gp2
    instance_type: m3.xlarge
  env:
    bosh:
      password: "<redacted>"
  name: bosh
  network: default
  stemcell:
    name: bosh-aws-xen-hvm-ubuntu-trusty-go_agent
    sha1: 971e869bd825eb0a7bee36a02fe2f61e930aaf29
    url: https://bosh.io/d/stemcells/bosh-aws-xen-hvm-ubuntu-trusty-go_agent?v=3232.6
...
Deploying
---------
Are you sure you want to deploy? (type 'yes' to continue): yes

Director task 144
  Started preparing deployment > Preparing deployment. Done (00:00:00)

  Started preparing package compilation > Finding packages to compile. Done (00:00:00)
...
Task 144 done

Started		2017-01-02 17:23:47 UTC
Finished	2017-01-02 17:34:46 UTC
Duration	00:10:59

Deployed 'dc01-staging-bosh' to 'dc01-proto-bosh'
```

This will take a little less time than **proto-BOSH** did (some packages were already compiled), and the next time you deploy, it go by much quicker, as all the packages should have been compiled by now (unless upgrading BOSH or the stemcell).

Once the deployment finishes, target the new BOSH Director to verify it works:

```
$ safe get secret/dc01/staging/bosh/users/admin # grab the admin user's password for bosh
$ bosh target https://10.4.32.4:25555 dc01-staging
Target set to 'dc01-staging-bosh'
Your username: admin
Enter password:
Logged in as 'admin'
```

Again, since our creds are already in the long-term vault, we can skip the credential migration that was done in the proto-bosh deployment and go straight to committing our new deployment to the repo, and pushing it upstream.

Next, we will deploy our jumpbox.

#### Beta Jumpbox

Unlike the proto jumpbox, which was initially deployed with Terraform, the beta (dev) jumpbox can be deployed via BOSH:

```
$ cd ~/ops/cf-deployments
$ genesis new deployment --template jumpbox
$ cd jumpbox-deployments
$ genesis new site --template openstack dc01
$ genesis new env dc01 dev
$ cd dc01/dev
$ make manifest
```

Similar to our other deployments, we'll start by using the errors from `make manifest` to tell us what values need to be supplied:

```
$ make manifest
Found stemcell bosh-openstack-kvm-ubuntu-trusty-go_agent 3312.15 on director
Found release jumpbox 4.2.3 on director
Found release toolbelt 3.2.10 on director
Found release shield 6.3.0 on director
4 error(s) detected:
 - $.meta.availability_zone: What availability zone should your jumpbox VMs be in?
 - $.networks.jumpbox.subnets: Specify your jumpbox subnet
 - $.properties.jumpbox.users: Set up some users to log into this jumpbox
 - $.properties.shield.agent.autoprovision: What is the URL to this jumpbox's shield installation?


Failed to merge templates; bailing...
Makefile:22: recipe for target 'manifest' failed
make: *** [manifest] Error 5
```

For `networking.yml`:

```
---
networks:
  - name: jumpbox
    subnets:
      - range: 10.4.16.0/24
        gateway: 10.4.16.1
        dns: [8.8.8.8, 8.8.4.4]
        cloud_properties:
          net_id: 20c35573-3a0c-4725-95a2-b58550407fcf   # <- Dev-Infra-0 Network UUID
        reserved:
          - 10.4.16.2 - 10.4.16.9
          - 10.4.16.20 - 10.4.16.254
        static:
          - 10.4.16.10

  - name: floating
    type: vip
    cloud_properties:
      net_id: 09b03d93-45f8-4bea-b3b8-7ad9169f23d5      # <- Public Network UUID
      security_groups: [wide-open]

jobs:
  - name: jumpbox
    networks:
    - name: jumpbox
      default: [dns, gateway]
    - name: floating
      static_ips:
      - 172.26.75.116              # <- Floating IP

cloud_provider:
  properties:
    openstack:
      default_security_groups: [default]
```

The Floating IP that you generated in Openstack and assign here is what you will later use to SSH into the jumpbox.

`properties.yml`:

```
---
meta:
  availability_zone: dc01

properties:
  shield:
    agent:
      autoprovision: https://10.4.1.32    # IP Address for SHIELD VM
```

The `credentials.yml` file is where you'll add your users:

```
---
meta:
  default_ssh_key: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDjqmzJtynAdxmcODCv0dtYmQrYk3lDeb03fjaQTRMtdEYbKnXk9ji8n3CHZj68JxUwxJRtQU70goP7a9X1PVQHJPIYmSWIdWbpeQdTjxymUh+kFjsu6ydlerLpnY3BjR5uWAxoQb+FRKuQDf9+gmkq95M65Lkef0scAXuVJPjLWhYnbZk6hJ4VlZIec6YKfw2x3+8fRM1sDDSWnazIfDQLudAmGnyeaVQsM5Qp7720imRQYPEl6QhwCgNFWhe42BwV5uXqQbBNVlRsoiu9hmGxjZIKB4f4E6uXfw1CPe0ZZh/34/W6CzN+kUwkWSgWet9+kS2Tf9vg0iqQDj/iFz3Cb/kKet0m7EcbYE51Y3fIC2EZAdlp5rQwDgyDoyz+x0IAPRgfMd9DXXjft/7phFdZp1SM4aBQ/bd5oYDpOTxhFZfHSGe4ZCh6tKX2ASzuP7Z4bhGlwZ50RQZqk5iYLsl+4g3Lt4XnjCz2oHgUHM5XVFiGMr7+PBqnQuWrDYJRcRAXwFZNh1dLRcj2ibYcemLWR31RfkYsEDTm6GbdjmV+XHkuvcqnkv7ZHx1MC2FhEELKLY2/LoU+8At8Fk2YU8JAfk9PROnCsQ8GjABZtEGBywHJxUXIMOFmj+9gJeHkbZsDQe6aas1z90HUKfK3u5AU5kC0e62RMjrtwb99eRK2+Q==

properties:
  jumpbox:
    users:
    - name: juser
      shell: /bin/bash
      setup_script: /var/vcap/packages/ruby-gems/bin/installer
      ssh_keys:
      - (( grab meta.default_ssh_key ))
```

Here we have the public key for `bosh` as the default SSH key. You could optionally add it to Vault like so:

```
$ safe set secret/dc01/dev/jumpbox/ssh/default pubkey@/full/path/to/bosh.pub
```

This creates the entry `secret/dc01/dev/jumpbox/ssh/default` with `pubkey` set to the contents of the file. You can view the secret with `safe get secret/dc01/dev/jumpbox/ssh/default:pubkey`.

You can then modify your `credentials.yml` as follows:

```
---
properties:
  jumpbox:
    users:
    - name: juser
      shell: /bin/bash
      setup_script: /var/vcap/packages/ruby-gems/bin/installer
      ssh_keys:
      - (( vault meta.vault_prefix "/ssh/default:pubkey" ))
```

Now that all of the errors are resolved, you can deploy with `make manifest deploy`. When you need to add additional users, simply update the `credentials.yml` file to mimic the above, whichever route you have chosen for storing / not storing the public key in Vault.

Note: although you need to also create an alpha (BOSH Lite) jumpbox for `genesis ci` / Concourse for the purposes of updating stemcells and releases, you only need the proto jumpbox and dev jumpboxes to access all of your environments. The proto jumpbox is intended to access the proto BOSH and BOSH Lite directors and the beta (dev) jumpbox is intended to access the dev BOSH director.

Now it's time to move on to deploying our `beta` (staging) Cloud Foundry!

#### Beta Cloud Foundry

To deploy Cloud Foundry, we will go back into our `ops` directory, making use of
the `cf-deployments` repo created when we built our alpha site:

```
$ cd ~/ops/cf-deployments
```

Also, make sure that you're targeting the right Vault, for good measure:

```
$ safe target proto
```

We will now create an `dc01` site for CF:

```
$ genesis new site --template openstack dc01
Created site dc01 (from template openstack):
~/ops/cf-deployments/dc01
├── README
└── site
    ├── disk-pools.yml
    ├── jobs.yml
    ├── networks.yml
    ├── properties.yml
    ├── releases
    ├── resource-pools.yml
    ├── stemcell
    │   ├── name
    │   └── version
    └── update.yml

2 directories, 10 files

```

And the `staging` environment inside it:

```
$ genesis new env dc01 staging

	proto       https://10.4.1.16:8200

	Use this Vault for storing deployment credentials?  [yes or no] yes
	Generating Cloud Foundry internal certs
	Uploading Cloud Foundry internal certs to Vault
	wrote secret/dc01/staging/cf-deployments/certs/internal_ca
	wrote secret/dc01/staging/cf-deployments/certs/consul_client
	wrote secret/dc01/staging/cf-deployments/certs/consul_server
	wrote secret/dc01/staging/cf-deployments/certs/etcd_client
	wrote secret/dc01/staging/cf-deployments/certs/etcd_server
	wrote secret/dc01/staging/cf-deployments/certs/etcd_peer
	wrote secret/dc01/staging/cf-deployments/certs/blobstore
	wrote secret/dc01/staging/cf-deployments/certs/uaa
	wrote secret/dc01/staging/cf-deployments/certs/bbs_client
	wrote secret/dc01/staging/cf-deployments/certs/bbs
	wrote secret/dc01/staging/cf-deployments/certs/rep_client
	wrote secret/dc01/staging/cf-deployments/certs/rep
	wrote secret/dc01/staging/cf-deployments/certs/doppler
	wrote secret/dc01/staging/cf-deployments/certs/metron
	wrote secret/dc01/staging/cf-deployments/certs/trafficcontroller
	Creating JWT Signing Key
	Creating app_ssh host key fingerprint
	Generating secrets
	Created environment dc01/staging:
	~/ops/cf-deployments-deployments/dc01/staging
	├── cloudfoundry.yml
	├── credentials.yml
	├── director.yml
	├── Makefile
	├── monitoring.yml
	├── name.yml
	├── networking.yml
	├── properties.yml
	├── README
	└── scaling.yml

	0 directories, 10 files
```

As you might have guessed, the next step will be to see what parameters we need to fill in:

```
$ cd dc01/staging
$ make manifest
71 error(s) detected:
 - $.meta.azs.z1: What availability zone should the *_z1 vms be placed in?
 - $.meta.azs.z2: What availability zone should the *_z2 vms be placed in?
 - $.meta.azs.z3: What availability zone should the *_z3 vms be placed in?
 - $.meta.cf.base_domain: Enter the Cloud Foundry base domain
 - $.meta.cf.blobstore_config.fog_connection.aws_access_key_id: What is the access key id for the blobstore S3 buckets?
 - $.meta.cf.blobstore_config.fog_connection.aws_secret_access_key: What is the secret key for the blobstore S3 buckets?
 - $.meta.cf.blobstore_config.fog_connection.host: What is the host name for the blobstore S3 buckets?
 - $.meta.cf.blobstore_config.fog_connection.port: What is the port for the blobstore S3 buckets?
 - $.meta.cf.directory_key_prefix: Replace the period in CF base domain with dash
 - $.meta.dns: Enter the DNS server for your VPC
 - $.meta.router_security_groups: Enter the security groups which should be applied to the gorouter VMs
 - $.meta.security_groups: Enter the security groups which should be applied to CF VMs
 - $.networks.cf1.subnets.0.cloud_properties.net_id: Enter the OpenStack Network ID for this subnet
 - $.networks.cf1.subnets.0.gateway: Enter the Gateway for this subnet
 - $.networks.cf1.subnets.0.range: Enter the CIDR address for this subnet
 - $.networks.cf1.subnets.0.reserved: Enter the reserved IP ranges for this subnet
 - $.networks.cf1.subnets.0.static: Enter the static IP ranges for this subnet
 - $.networks.cf2.subnets.0.cloud_properties.net_id: Enter the OpenStack Network ID for this subnet
 - $.networks.cf2.subnets.0.gateway: Enter the Gateway for this subnet
 - $.networks.cf2.subnets.0.range: Enter the CIDR address for this subnet
 - $.networks.cf2.subnets.0.reserved: Enter the reserved IP ranges for this subnet
 - $.networks.cf2.subnets.0.static: Enter the static IP ranges for this subnet
 - $.networks.cf3.subnets.0.cloud_properties.net_id: Enter the OpenStack Network ID for this subnet
 - $.networks.cf3.subnets.0.gateway: Enter the Gateway for this subnet
 - $.networks.cf3.subnets.0.range: Enter the CIDR address for this subnet
 - $.networks.cf3.subnets.0.reserved: Enter the reserved IP ranges for this subnet
 - $.networks.cf3.subnets.0.static: Enter the static IP ranges for this subnet
 - $.networks.router1.subnets.0.cloud_properties.net_id: Enter the OpenStack Network ID for this subnet
 - $.networks.router1.subnets.0.gateway: Enter the Gateway for this subnet
 - $.networks.router1.subnets.0.range: Enter the CIDR address for this subnet
 - $.networks.router1.subnets.0.reserved: Enter the reserved IP ranges for this subnet
 - $.networks.router1.subnets.0.static: Enter the static IP ranges for this subnet
 - $.networks.router2.subnets.0.cloud_properties.net_id: Enter the OpenStack Network ID for this subnet
 - $.networks.router2.subnets.0.gateway: Enter the Gateway for this subnet
 - $.networks.router2.subnets.0.range: Enter the CIDR address for this subnet
 - $.networks.router2.subnets.0.reserved: Enter the reserved IP ranges for this subnet
 - $.networks.router2.subnets.0.static: Enter the static IP ranges for this subnet
 - $.networks.runner1.subnets.0.cloud_properties.net_id: Enter the OpenStack Network ID for this subnet
 - $.networks.runner1.subnets.0.gateway: Enter the Gateway for this subnet
 - $.networks.runner1.subnets.0.range: Enter the CIDR address for this subnet
 - $.networks.runner1.subnets.0.reserved: Enter the reserved IP ranges for this subnet
 - $.networks.runner1.subnets.0.static: Enter the static IP ranges for this subnet
 - $.networks.runner2.subnets.0.cloud_properties.net_id: Enter the OpenStack Network ID for this subnet
 - $.networks.runner2.subnets.0.gateway: Enter the Gateway for this subnet
 - $.networks.runner2.subnets.0.range: Enter the CIDR address for this subnet
 - $.networks.runner2.subnets.0.reserved: Enter the reserved IP ranges for this subnet
 - $.networks.runner2.subnets.0.static: Enter the static IP ranges for this subnet
 - $.networks.runner3.subnets.0.cloud_properties.net_id: Enter the OpenStack Network ID for this subnet
 - $.networks.runner3.subnets.0.gateway: Enter the Gateway for this subnet
 - $.networks.runner3.subnets.0.range: Enter the CIDR address for this subnet
 - $.networks.runner3.subnets.0.reserved: Enter the reserved IP ranges for this subnet
 - $.networks.runner3.subnets.0.static: Enter the static IP ranges for this subnet
 - $.properties.cc.buildpacks.fog_connection.aws_access_key_id: What is the access key id for the blobstore S3 buckets?
 - $.properties.cc.buildpacks.fog_connection.aws_secret_access_key: What is the secret key for the blobstore S3 buckets?
 - $.properties.cc.buildpacks.fog_connection.host: What is the host name for the blobstore S3 buckets?
 - $.properties.cc.buildpacks.fog_connection.port: What is the port for the blobstore S3 buckets?
 - $.properties.cc.droplets.fog_connection.aws_access_key_id: What is the access key id for the blobstore S3 buckets?
 - $.properties.cc.droplets.fog_connection.aws_secret_access_key: What is the secret key for the blobstore S3 buckets?
 - $.properties.cc.droplets.fog_connection.host: What is the host name for the blobstore S3 buckets?
 - $.properties.cc.droplets.fog_connection.port: What is the port for the blobstore S3 buckets?
 - $.properties.cc.packages.fog_connection.aws_access_key_id: What is the access key id for the blobstore S3 buckets?
 - $.properties.cc.packages.fog_connection.aws_secret_access_key: What is the secret key for the blobstore S3 buckets?
 - $.properties.cc.packages.fog_connection.host: What is the host name for the blobstore S3 buckets?
 - $.properties.cc.packages.fog_connection.port: What is the port for the blobstore S3 buckets?
 - $.properties.cc.resource_pool.fog_connection.aws_access_key_id: What is the access key id for the blobstore S3 buckets?
 - $.properties.cc.resource_pool.fog_connection.aws_secret_access_key: What is the secret key for the blobstore S3 buckets?
 - $.properties.cc.resource_pool.fog_connection.host: What is the host name for the blobstore S3 buckets?
 - $.properties.cc.resource_pool.fog_connection.port: What is the port for the blobstore S3 buckets?
 - $.properties.cc.security_group_definitions.load_balancer.rules: Specify the rules for allowing access for CF apps to talk to the CF Load Balancer External IPs
 - $.properties.cc.security_group_definitions.services.rules: Specify the rules for allowing access to CF services subnets
 - $.properties.cc.security_group_definitions.user_bosh_deployments.rules: Specify the rules for additional BOSH user services that apps will need to talk to


Failed to merge templates; bailing...
Makefile:22: recipe for target 'manifest' failed
make: *** [manifest] Error 5
```

Oh boy. That's a lot. Cloud Foundry must be complicated. Looks like a lot of the fog_connection properties are all duplicates though, so lets fill out `properties.yml` with those (no need to create the blobstore S3 buckets yourself):

`properties.yml`

```
---
meta:
  type: cf
  site: dc01
  env: dev
  skip_ssl_validation: true

  cf:
    base_domain: 192.168.10.154.sslip.io
    directory_key_prefix: 192-168-10-154-sslip-io
    blobstore_config:
      fog_connection:
        aws_access_key_id: (( vault "secret/s3:access_key" ))
        aws_secret_access_key: (( vault "secret/s3:secret_key" ))
        scheme: http
        # host: (( get "object." + meta.openstack.domain ))
        host: 192.168.8.168
        port: 8080
        # Required
        path_style: true
        # v4 is buggy at the moment, stick with v2 for now
        aws_signature_version: 2
        provider: "AWS"
        #region: dc01
properties:
  bolo:
    submission:
      address: 10.4.1.65
    collectors:
      - { every: 20s, run: 'linux' }
  loggregator:
    servers:
    - 10.4.20.105
  loggregator_endpoint:
    host: 10.4.20.105
    port: 3456
```

Also, let's fill out `scaling.yml` so we can more easily scale out our Availability Zones and jobs:

```
meta:
  azs:
    z1: dc01
    z2: dc01
    z3: dc01
jobs:
 - name: access_z1
   instances: 1
 - name: access_z2
   instances: 0
 - name: api_z1
   instances: 1
 - name: api_z2
   instances: 0
 - name: brain_z1
   instances: 1
 - name: brain_z2
   instances: 0
 - name: cc_bridge_z1
   instances: 1
 - name: cc_bridge_z2
   instances: 0
 - name: cell_z1
   instances: 1
 - name: cell_z2
   instances: 0
 - name: doppler_z1
   instances: 1
 - name: doppler_z2
   instances: 0
 - name: loggregator_trafficcontroller_z1
   instances: 1
 - name: loggregator_trafficcontroller_z2
   instances: 0
 - name: route_emitter_z1
   instances: 1
 - name: route_emitter_z2
   instances: 0
 - name: router_z1
   instances: 1
 - name: router_z2
   instances: 0
 - name: stats
   instances: 1
 - name: uaa_z1
   instances: 1
 - name: uaa_z2
   instances: 0
```

Time to start building out the `networking.yml` file. Let's consult our [Network Plan][netplan] for the subnet information, cross referencing with Terraform output to get the subnet IDs:

```
---
meta:
  azs:
    z1: dc01
    z2: dc01
    z3: dc01
  dns: [8.8.8.8, 8.8.4.4]
  router_security_groups: [wide-open]
  security_groups: [wide-open]

networks:
- name: router1
  subnets:
  - range: 10.4.19.0/25
    static: [10.4.19.4 - 10.4.19.10]
    reserved:
      - 10.4.19.2 - 10.4.19.3
      - 10.4.19.120 - 10.4.19.126
    gateway: 10.4.19.1
    cloud_properties:
      net_id: 262fb235-de6c-4979-832a-225c66859d26
- name: router2
  subnets:
  - range: 10.4.19.128/25
    static: [10.4.19.132 - 10.4.19.138]
    reserved:
      - 10.4.19.130 - 10.4.19.131
      - 10.4.19.248 - 10.4.19.254
    gateway: 10.4.19.129
    cloud_properties:
      net_id: 41fb3f7d-9198-49e2-84b9-d142628a666a
- name: cf1
  subnets:
  - range: 10.4.20.0/24
    static: [10.4.20.4 - 10.4.20.100]
    reserved: [10.4.20.2 - 10.4.20.3]
    gateway: 10.4.20.1
    cloud_properties:
      net_id: bbea24c9-58dc-4df5-899d-fd46e3dfbe5e
- name: cf2
  subnets:
  - range: 10.4.21.0/24
    static: [10.4.21.4 - 10.4.21.100]
    reserved: [10.4.21.2 - 10.4.21.3]
    gateway: 10.4.21.1
    cloud_properties:
      net_id: a2f1e561-2c02-4f7b-9dd9-4c5fd44c9783
- name: cf3
  subnets:
  - range: 10.4.22.0/24
    static: [10.4.22.4 - 10.4.22.100]
    reserved: [10.4.22.2 - 10.4.22.3]
    gateway: 10.4.22.1
    cloud_properties:
      net_id: 4e1df0ff-f7f9-4cf8-9c19-77afd48e7e9f
- name: runner1
  subnets:
  - range: 10.4.23.0/24
    static: [10.4.23.4 - 10.4.23.100]
    reserved: [10.4.23.2 - 10.4.23.3]
    gateway: 10.4.23.1
    cloud_properties:
      net_id: 7f4b6f05-685f-48c7-bc6d-edc8bd00b145
- name: runner2
  subnets:
  - range: 10.4.24.0/24
    static: [10.4.24.4 - 10.4.24.100]
    reserved: [10.4.24.2 - 10.4.24.3]
    gateway: 10.4.24.1
    cloud_properties:
      net_id: 631b5c2d-7948-4e77-8ca7-36417514e835
- name: runner3
  subnets:
  - range: 10.4.25.0/24
    static: [10.4.25.4 - 10.4.25.100]
    reserved: [10.4.25.2 - 10.4.25.3]
    gateway: 10.4.25.1
    cloud_properties:
      net_id: ecbf813d-77af-4919-9c80-279d9eaf10c6

- name: floating
  type: vip
  cloud_properties:
    net_id: 09b03d93-45f8-4bea-b3b8-7ad9169f23d5
    security_groups: [wide-open]
jobs:
- name: api_z1
  networks:
  - name: cf1
    default: [dns, gateway]
  - name: floating
    static_ips:
    - 192.168.10.125
- name: api_z2
  networks:
  - name: cf2
    default: [dns, gateway]

properties:
  cc:
    security_group_definitions:
    - name: load_balancer
      rules: []
    - name: services
      rules:
      - destination: 10.4.26.0-10.4.28.255
        protocol: all
    - name: user_bosh_deployments
      rules: []
```
That should be it, finally. Let's deploy!

```
$ make deploy
RSA 1024 bit CA certificates are loaded due to old openssl compatibility
Acting as user 'admin' on 'dc01-staging-bosh'
Checking whether release cf/250 already exists...NO
Using remote release 'https://bosh.io/d/github.com/cloudfoundry/cf-release?v=250'

Director task 6
  Started downloading remote release > Downloading remote release
...
Deploying
---------
Are you sure you want to deploy? (type 'yes' to continue): yes
...

Started		2017-01-02 17:23:47 UTC
Finished	2017-01-02 17:34:46 UTC
Duration	00:10:59

Deployed 'dc01-staging-cf' to 'dc01-staging-bosh'

```

If you want to scale your deployment in the current environment (here it is staging), you can modify `scaling.yml` in your `cf-deployments/dc01/staging` directory. In the following example, you scale runners in both AZ to 2. Afterwards you can run `make manifest` and `make deploy`, but always remember to verify your changes in the manifest before you type `yes`.

```
jobs:

- name: runner_z1
  instances: 2

- name: runner_z2
  instances: 2

```
To make the manifest and deploy the changes run `make manifest deploy`. Always make sure to verify the detected changes match what you intended in the manifest before entering `yes` to kickoff the deploy.

After a long while of compiling and deploying VMs, your Cloud Foundry should now be up, and accessible! You can check the sanity by running the smoke tests with `genesis bosh run errand smoke_tests`. 

To target your Cloud Foundry to start making orgs, spaces, and pushing apps use:

```
cf login -a https://api.system.192.168.10.154.sslip.io
```

The admin user's password can be retrieved from Vault with `safe get secret/dc01/dev/cf-cloudfoundry/creds/users/admin:password`. If you run into any trouble, make sure that your DNS is pointing properly to the correct Load Balancer for this environment and that the Load Balancer has the correct SSL certificate for your site.

##### Push An App to Beta Cloud Foundry

After you successfully deploy the Beta CF, you can push an simple app to learn more about CF. In the CF world, every application and service is scoped to a space. A space is inside an org and provides users with access to a shared location for application development, deployment, and maintenance. An org is a development account that an individual or multiple collaborators can own and use. You can click [orgs, spaces, roles and permissions][orgs and spaces] to learn more  details.

The first step is creating and org and an space and targeting the org and space you created by running the following commands.

```
cf create-org sw-codex
cf target -o sw-codex
cf create-space test
cf target -s test

```

Once you are in the space, you can push an very simple app [cf-env][cf-env]  to the CF. Clone the [cf-env][cf-env]  repo on your bastion server, then go inside the `cf-env` directory, simply run `cf push` and it will start to upload, stage and run your app.

Your `cf push` command may fail like this:

```
Using manifest file /home/user/cf-env/manifest.yml

Updating app cf-env in org sw-codex / space test as admin...
OK

Uploading cf-env...
FAILED
Error processing app files: Error uploading application.
Server error, status code: 500, error code: 10001, message: An unknown error occurred.

```
You can try to debug this yourself for a while or find the possible solution in [Debug Unknown Error When You Push Your APP to CF][DebugUnknownError].

#### Beta haproxy
For the haproxy/(-ies) in beta environments you'll need to create a Floating IP for each environment. For this deployment, we'll be using `192.168.10.154` as the Floating IP.

Once you have the Floating IP, create the new environment:
```
$ cd ~/ops/haproxy-deployments
$ genesis new site --template openstack dc01
$ genesis new dc01 dev 
$ cd dc01/dev
$ make manifest
```

```
...
8 error(s) detected:
 - $.meta.azs.z1: What availability zone should the *_z1 vms be placed in?
 - $.meta.azs.z2: What availability zone should the *_z2 vms be placed in?
 - $.meta.azs.z3: What availability zone should the *_z3 vms be placed in?
 - $.networks.haproxy1.subnets: Please specify haproxy1 subnets
 - $.networks.haproxy2.subnets: Please specify haproxy1 subnets
 - $.properties.ha_proxy.backend_servers: Specify your go routers IPs as backend servers
 - $.properties.ha_proxy.ssl_pem: Configure the ssl pem you use for your haproxy
 - $.properties.ha_proxy.tcp.cf_app_ssh.backend_servers: Specify you  CF Access VMs IPs as your backend servers
```

Now add the network information to `networking.yml`:

```
--
meta:
  azs:
    z1: dc01
    z2: (( grab meta.azs.z1 ))
    z3: (( grab meta.azs.z1 ))
  dns: [8.8.8.8, 8.8.4.4]
  router_security_groups: [wide-open]
  security_groups: [wide-open]

networks:
- name: haproxy1
  type: manual
  subnets:
  - range: 10.4.19.0/25
    reserved:
    - 10.4.19.2 - 10.4.19.8
    static: [ 10.4.19.9 - 10.4.19.12 ]
    gateway: 10.4.19.1
    cloud_properties:
      net_id: 262fb235-de6c-4979-832a-225c66859d26    # Network UUID for dev-cf-edge-0
      security_groups: [ wide-open ]

- name: haproxy2
  type: manual
  subnets:
  - range: 10.4.19.128/25
    reserved:
    - 10.4.19.129 - 10.4.19.135
    static: [ 10.4.19.136 - 10.4.19.140 ]
    gateway: 10.4.19.129
    cloud_properties:
      net_id: 41fb3f7d-9198-49e2-84b9-d142628a666a    # Network UUID for dev-cf-edge-1
      security_groups: [ wide-open ]

- name: floating
  type: vip
  cloud_properties:
    net_id: 09b03d93-45f8-4bea-b3b8-7ad9169f23d5      # Network UUID for public
    security_groups: [wide-open]

jobs:
  - name: haproxy_z1
    networks:
    - name: haproxy1
      default: [dns, gateway]
    - name: floating
      static_ips:
      - 192.168.10.154                                 # Floating IP for haproxy
```
And also the `properties.yml`:

```
---
properties:
  ha_proxy:
    ssl_pem:
      - (( vault meta.vault_prefix "/certs/pems:192.168.10.154.sslip.io" ))
    backend_servers: [ 10.4.19.4, 10.4.19.132 ]  # Dev Cloud Foundry router_z* IPs

    tcp:
      - name: cf_app_ssh
        backend_servers: [ 10.4.20.108 ]         # Dev Cloud Foundry access_z1 IP
```

As before, we'll need to generate the haproxy cert. In this case, we have a true Floating IP so we will be using that IP to generate the cert:

```
$ ENV_PATH=secret/dc01/dev FIP=192.168.10.154 ./bin/haproxy_cert_gen
```

As a reminder if you are using a domain with its own CA, you will generate the cert with your own internal process and upload it to Vault using `safe write`. For example, if you had a cert for `example.com` for this environment and saved it as `haproxy.pem`, you would use:

```
$ safe write secret/dc01/dev/haproxy/certs/pems example.com@haproxy.pem
```

You would then reference the secret in the manifest with `(( vault meta.vault_prefix "/certs/pems:example.com" ))`.

After configuring all the templates, and generating the certificate, make the manifest and deploy with `make manifest deploy`.

There is one final step: since Cloud Foundry was already deployed so we could use it's CA, you'll need to edit `properties.yml` haproxy Floating IP for the base domain and directory key prefix:

```
  cf:
    base_domain: 192.168.10.154.sslip.io
    directory_key_prefix: 192-168-10-154-sslip-io
```

Once this is done remake the manifest and deploy the Beta (dev) Cloud Foundry with `make manifest deploy`. You will also need to delete the original `run` and `system` domains by listing the domains with `cf domains` and then using `cf delete-shared-domain` and `cf delete-domain`. Alternatively, if the Cloud Foundry has yet to have any apps/etc. added to it you may want to delete the deployment with `bosh delete deployment dc01-dev-cf-cloudfoundry` and then deploy normally.

### Production Environment

Deploying the production environment will be much like deploying the `beta` environment above. You will need to deploy a BOSH Director, Cloud Foundry, and any services also deployed in the `beta` site. Hostnames, credentials, network information, and possibly scaling parameters will all be different, but the procedure for deploying them is the same.

## Logging with Sawmill

Sawmill is a BOSH release that aggregates logs to assist with troubleshooting Cloud Foundry and its services that support [nxlog][nxlog]. Although Sawmill logs can be viewed using `curl`, we recommend installing `logemongo` - a CLI tool for Sawmill. If you need to store your logs for later access, we recommend using a storage bucket (e.g. S3).

Since we only have Cloud Foundries in Alpha (BOSH Lite) and Dev, we'll only be deploying Sawmill to those two environments here.

### How to deploy

To create the deployment:

```
$ genesis new deployment --template sawmill
$ cd sawmill-deployments
```

For the Beta (Dev) Environment:


```
$ genesis new site --template openstack dc01
$ genesis new env dc01 dev
$ cd dc01/dev
$ make manifest
```

As before, we'll need to supply the information requested in the errors:



```
$ make manifest
Found stemcell bosh-openstack-kvm-ubuntu-trusty-go_agent latest on director
Found release sawmill 2.1.0 on director
Found release toolbelt latest on director
8 error(s) detected:
 - $.meta.openstack.azs.z1: Define the z1 Openstack availability zone
 - $.networks.sawmill_z1.subnets.0.cloud_properties.net_id: Enter the network ID Openstack for the Dev-Infra-0 network (networks.0<sawmill_z1>.subnets.cloud_properties.net_id))
 - $.networks.sawmill_z1.subnets.0.gateway: Enter the Gateway for this subnet (networks.0<sawmill_z1>.subnets.gateway)
 - $.networks.sawmill_z1.subnets.0.range: Enter the CIDR address for this subnet (networks.0<sawmill_z1>.subnets.range)
 - $.networks.sawmill_z1.subnets.0.reserved: Enter the reserved IP ranges for this subnet (networks.0<sawmill_z1>.subnets.reserved)
 - $.networks.sawmill_z1.subnets.0.static: Enter the static IP ranges for this subnet (networks.0<sawmill_z1>.subnets.static)
 - $.properties.sawmill.skip_ssl_verify: Specify whether or not to skip SSL verification (properties.sawmill.skip_ssl_verify)
 - $.properties.sawmill.users: Specify admin user name & password (properties.sawmill.users.0.{name,pass})


Failed to merge templates; bailing...
Makefile:22: recipe for target 'manifest' failed
make: *** [manifest] Error 5
```

`networking.yml`:

```
---
meta:
  openstack:
    azs:
      z1: dc01

networks:
  - name: sawmill_z1
    type: manual
    subnets:
    - range: 10.4.16.0/24
      gateway: 10.4.16.1
      dns: [8.8.8.8, 8.8.4.4]
      reserved:
        - 10.4.16.2 - 10.4.16.19
        - 10.4.16.30 - 10.4.16.254
      static:
        - 10.4.16.20
      cloud_properties:
        net_id: 20c35573-3a0c-4725-95a2-b58550407fcf   # <- Dev-Infra-0 Network UUID
        security_groups: [wide-open]
```
`credentials.yml`:

```
---
properties:
  sawmill:
    users:
      - name: admin
        pass: (( vault meta.vault_prefix "/users/admin:password" ))
```

`properties.yml`:

```
---
properties:
  sawmill:
    skip_ssl_verify: true
```

You will need to add the user to Vault and assign a password. The easiest way to do this is:

```
safe gen secret/dc01/dev/sawmill/users/admin password

```

You can view the sawmill credentials tree in Vault with `safe`:

```
safe tree secret/dc01/dev/sawmill
```

You can view the password with `safe get secret/dc01/dev/sawmill/users/admin:password`. Now you can make the manifest and deploy:

```
$ make manifest deploy
```

For the **Alpha** (BOSH Lite) Environment:

Repeat the above process for both alpha (BOSH Lite) and dev. Since BOSH Lite is it's own site, and not in `dc01`, make sure to use the `bosh-lite` template for that deployment e.g. assuming you're in the `sawmill-deployments` directory:

```
$ genesis new site --template bosh-lite bosh-lite
$ genesis new env bosh-lite alpha
$ cd bosh-lite/alpha
$ make manifest
```

In this case both `credentials.yml` and `properties.yml` will match the Dev deployment, so we only need to add `networking.yml`:

```
---
networks:
  - name: sawmill_z1
    type: manual
    subnets:
    - range: 10.244.8.0/24
      gateway: 10.244.8.1
      dns: [8.8.8.8, 8.8.4.4]
      reserved:
        - 10.244.8.2 - 10.244.8.19
        - 10.244.8.30 - 10.244.8.254
      static:
        - 10.244.8.20
```

And create the `user:password` in Vault:

```
safe gen secret/bosh-lite/alpha/sawmill/users/admin password
```

Now you can make the manifest and deploy:

```
$ make manifest deploy
```

### Viewing Logs with Logemongo

First, log into a jumpbox that can access the private network of the environment where you want to view logs (e.g. the dev jumpbox for the dev environment). Then, grab `[logemongo](logemongo)` from Github:

```
sudo curl -o /usr/local/bin/logemongo \
  https://raw.githubusercontent.com/starkandwayne/logemongo/master/logemongo
sudo chmod 0755 /usr/local/bin/logemongo
```

Since our jumpboxes use Ubuntu, we'll need to grab a couple packages for the utility:

```
sudo apt-get install libio-socket-ssl-perl libconfig-yaml-perl
```

You can view the streaming logs from the appropriate Sawmill using the host flag and passing through the username and password. Since we're using self-signed certificates, we'll also need to disable SSL verification. e.g. to stream logs in dev:

```
$ logemongo -H 10.4.16.20 -u admin -p $(safe get secret/dc01/dev/sawmill/users/admin:password) --no-ssl
```

Depending on log volume, it may take a few moments for logs to begin to appear. If you wish to stream logs only from a certain source, or exclude logs only from a certain source, you can use `-i` and `-x` respectively with a regex pattern. You can also limit the number of lines of output with `-c`.

### Next Steps

Lather, rinse, repeat for all additional environments (e.g. loadtest, production).

[//]: # (Links, please keep in alphabetical order)

[bolo]:              https://github.com/cloudfoundry-community/bolo-boshrelease
[cf-env]:            https://github.com/cloudfoundry-community/cf-env
[cfconsul]:          https://docs.cloudfoundry.org/concepts/architecture/#bbs-consul
[cfetcd]:            https://docs.cloudfoundry.org/concepts/architecture/#etcd
[DebugUnknownError]: http://www.starkandwayne.com/blog/debug-unknown-error-when-you-push-your-app-to-cf/
[DRY]:               https://en.wikipedia.org/wiki/Don%27t_repeat_yourself
[fly]:               https://concourse.ci/fly-cli.html
[genesis]:           https://github.com/starkandwayne/genesis
[infra-ips]:         https://github.com/starkandwayne/codex/blob/master/part3/network.md#global-infrastructure-ip-allocation
[jumpbox]:           https://github.com/starkandwayne/jumpbox
[logemongo]:         https://github.com/starkandwayne/logemongo
[netplan]:           https://github.com/starkandwayne/codex/blob/master/network.md
[ngrok-download]:    https://ngrok.com/download
[infra-ips]:         https://github.com/starkandwayne/codex/blob/master/part3/network.md#global-infrastructure-ip-allocation
[nxlog]:             https://github.com/hybris/nxlog-boshrelease
[orgs and spaces]:   https://docs.cloudfoundry.org/concepts/roles.html
[shield]:            https://github.com/starkandwayne/shield
[slither]:           http://slither.io
[spruce-129]:        https://github.com/geofffranks/spruce/issues/129
[troubleshooting]:   troubleshooting.md
[verify_ssh]:        https://github.com/starkandwayne/codex/blob/master/troubleshooting.md#verify-keypair

[//]: # (Images, put in /images folder)

[levels_of_bosh]:         images/levels_of_bosh.png "Levels of Bosh"
[bastion_host_overview]:  images/bastion_host_overview.png "Bastion Host Overview"
[bastion_1]:              images/bastion_step_1.png "vault-init"
[bastion_2]:              images/bastion_step_2.png "proto-BOSH"
[bastion_3]:              images/bastion_step_3.png "Vault"
[bastion_4]:              images/bastion_step_4.png "Shield"
[bastion_5]:              images/bastion_step_5.png "Bolo"
[bastion_6]:              images/bastion_step_6.png "Concourse"
[global_network_diagram]: images/global_network_diagram.png "Global Network Diagram"
[shield_ui]:              images/shield_ui.png "SHIELD UI"
[pipelines]:              images/pipelines.png "pipelines"
[manual_deploy]:          images/manual-deployment.png "manual-deploy"
[bosh_pipeline]:          images/bosh-pipeline.png "bosh-pipeline"

